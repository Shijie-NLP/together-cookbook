{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "73b0444b",
      "metadata": {
        "id": "73b0444b"
      },
      "source": [
        "# Using Toolhouse for Tool Use with Together"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "626a645c",
      "metadata": {
        "id": "626a645c"
      },
      "source": [
        "[Toolhouse](https://app.toolhouse.ai/) is the first complete infrastructure platform for building, running and managing tool use.\n",
        "\n",
        "With Toolhouse, you can equip your LLM with extra skills (also known as tools).\n",
        "\n",
        "Some of the most popular tools include:\n",
        "- scraping data from the web or social media\n",
        "- generate images\n",
        "- compile or execute code and return the values\n",
        "\n",
        "This cookbook demonstrates how you can **equip LLMs running on** [Together.ai](https://together.ai/) - **with tools**, without the need for your to code or prompt these tools.\n",
        "\n",
        "In this short demo, we'll show how we can equip an LLM to generate images from real-data.\n",
        "\n",
        "We'll use Toolhouse with the model tagged `meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo` and hosted on Together's infrastructure.\n",
        "\n",
        "This model is fine-tuned for effective and precise tool use."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "002c27a7",
      "metadata": {
        "id": "002c27a7"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install toolhouse openai"
      ],
      "metadata": {
        "id": "LaMYsh-3580q",
        "collapsed": true
      },
      "id": "LaMYsh-3580q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "d237c86c",
      "metadata": {
        "id": "d237c86c"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from toolhouse import Toolhouse"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "458ace84",
      "metadata": {
        "id": "458ace84"
      },
      "source": [
        "To integrate Together and Toolhouse, you'll need to set up two environment variables: `TOGETHER_API_KEY` and `TOOLHOUSE_API_KEY`. Follow these steps to obtain your API keys:\n",
        "\n",
        "* **Together API Key**: Get your key by visiting the [Together Console](https://api.together.ai/).\n",
        "* **Toolhouse API Key**: Sign up for Toolhouse using [this link](https://join.toolhouse.ai) to receive $150 in credits. You will receive your API key as part of the onboarding step, and you can always, navigate to the [Toolhouse API Keys page](https://app.toolhouse.ai/settings/api-keys) to create and get an API key.\n",
        "* **Install the X tool**: In your [Toolhouse dashboard](https://app.toolhouse.ai), click Install next to the \"Search X\" tool ([direct link](https://app.toolhouse.ai/store/search_x)).\n",
        "* **Install the Image Generation tool**: In your [Toolhouse dashboard](https://app.toolhouse.ai), click Install next to the \"Image Generation\" tool ([direct link](https://app.toolhouse.ai/store/image_generation_flux)).\n",
        "\n",
        "Once you have both API keys, set them as environment variables to start using Together and Toolhouse.\n",
        "\n",
        "To do that click on the Key icon in the left menu on Colab or define them in your environment variables and use them with `os.getenv()`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools=th.get_tools(\"weather\")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that can access external functions. The responses from these function calls will be appended to this dialogue. Please provide responses based on the information from these function calls.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is the current temperature of New York?\"}\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "    messages=messages,\n",
        "    tools=tools,\n",
        "    tool_choice=\"auto\",\n",
        ")\n",
        "\n",
        "messages += th.run_tools(response)\n",
        "\n",
        "final_response = client.chat.completions.create(\n",
        "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "    messages=messages,\n",
        "    tools=tools,\n",
        "    tool_choice=\"auto\",\n",
        ")\n",
        "\n",
        "print(final_response.choices[0].message.content)\n",
        "\n",
        "\n",
        "# if tool_calls:\n",
        "#     for tool_call in tool_calls:\n",
        "#         function_name = tool_call.function.name\n",
        "#         function_args = json.loads(tool_call.function.arguments)\n",
        "\n",
        "#         if function_name == \"get_current_weather\":\n",
        "#             function_response = get_current_weather(\n",
        "#                 location=function_args.get(\"location\"),\n",
        "#                 unit=function_args.get(\"unit\"),\n",
        "#             )\n",
        "#             messages.append(\n",
        "#                 {\n",
        "#                     \"tool_call_id\": tool_call.id,\n",
        "#                     \"role\": \"tool\",\n",
        "#                     \"name\": function_name,\n",
        "#                     \"content\": function_response,\n",
        "#                 }\n",
        "#             )\n",
        "\n",
        "#     function_enriched_response = client.chat.completions.create(\n",
        "#         model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "#         messages=messages,\n",
        "#     )\n",
        "#     print(json.dumps(function_enriched_response.choices[0].message.model_dump(), indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQyux-qk3_5B",
        "outputId": "a6f555ed-9334-4ed6-cd33-c457f9cc026e"
      },
      "id": "QQyux-qk3_5B",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've imported the SDKs, now let's initialize them and authenticate ourselves using an API key. One per service respectively. Your API keys can be found in the left sidebar of Google Colab after clicking the key icon."
      ],
      "metadata": {
        "id": "_kblVg3pXlMy"
      },
      "id": "_kblVg3pXlMy"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "bf32deff",
      "metadata": {
        "id": "bf32deff"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "client = OpenAI(base_url = \"https://api.together.xyz/v1\", api_key=userdata.get(\"TOGETHER_API_KEY\"))\n",
        "\n",
        "th = Toolhouse(api_key=userdata.get('TOOLHOUSE_API_KEY'), provider=\"openai\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32220c3f",
      "metadata": {
        "id": "32220c3f"
      },
      "source": [
        "We will use a Meta model specificially the Instruct flavor:\n",
        "- [meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\n",
        " model](https://docs.together.ai/docs/function-calling#supported-models).\n",
        "\n",
        "This model is based on the Meta 3.1 70B model and was fine-tuned for tool use and function calling:\n",
        "\n",
        "We can test that we can call the Mixtral model hosted on Together by running the cell below. It should output some info about NYC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "8771ddd7",
      "metadata": {
        "id": "8771ddd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe577d9a-1644-4858-9cf0-b4a84ae42118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New York, also known as the \"Empire State,\" is a state located in the northeastern United States. It is the 27th largest state by area and the 4th most populous state in the country. Here are some interesting facts and attractions about New York:\n",
            "\n",
            "**Cities:**\n",
            "\n",
            "1. New York City: The most populous city in the United States, known for its iconic skyline, Broadway shows, and world-class museums like the Metropolitan Museum of Art and the Museum of Modern Art (MoMA).\n",
            "2. Buffalo: Located in western New York, known for its rich history, cultural attractions, and natural beauty, including Niagara Falls.\n",
            "3. Albany: The capital city of New York, known for its historic architecture, cultural attractions, and the New York State Capitol building.\n",
            "4. Rochester: Located in western New York, known for its rich history, cultural attractions, and the George Eastman Museum.\n",
            "\n",
            "**Attractions:**\n",
            "\n",
            "1. Statue of Liberty: A iconic symbol of freedom and democracy, located in New York Harbor.\n",
            "2. Central Park: A large public park in Manhattan, offering a peaceful escape from the hustle and bustle of the city.\n",
            "3. Times Square: A bustling area in Manhattan known for its bright lights, giant billboards, and lively atmosphere.\n",
            "4. Niagara Falls: A breathtaking natural wonder located on the border of New York and Ontario, Canada.\n",
            "5. The High Line: An elevated park built on an old rail line, offering stunning views of the Hudson River and the city.\n",
            "6. Coney Island: A popular beach destination in Brooklyn, known for its amusement parks, boardwalk, and classic eateries.\n",
            "7. The Bronx Zoo: The largest metropolitan zoo in the United States, home to over 6,000 animals from around the world.\n",
            "8. The 9/11 Memorial & Museum: A poignant tribute to the victims of the 9/11 attacks, located at the World Trade Center site.\n",
            "\n",
            "**Food:**\n",
            "\n",
            "1. New York-style pizza: A classic style of pizza known for its thin crust, foldable slices, and flavorful toppings.\n",
            "2. Bagels: A staple of New York cuisine, often topped with cream cheese, lox, or other spreads.\n",
            "3. Hot dogs: A popular street food in New York, often served with sauerkraut, mustard, and grilled onions.\n",
            "4. Pastrami sandwiches: A classic deli dish, often served with mustard and pickles.\n",
            "5. Apple cider: A popular drink in New York, especially during the fall season.\n",
            "\n",
            "**Sports:**\n",
            "\n",
            "1. New York Yankees: A professional baseball team based in the Bronx, known for their iconic pinstripes and 27 World Series championships.\n",
            "2. New York Mets: A professional baseball team based in Queens, known for their blue and orange uniforms and two World Series championships.\n",
            "3. New York Giants: A professional football team based in East Rutherford, New Jersey, known for their four Super Bowl championships.\n",
            "4. New York Jets: A professional football team based in East Rutherford, New Jersey, known for their green and white uniforms and one Super Bowl championship.\n",
            "5. New York Knicks: A professional basketball team based in Manhattan, known for their orange and blue uniforms and two NBA championships.\n",
            "\n",
            "**Education:**\n",
            "\n",
            "1. Columbia University: A prestigious Ivy League university located in Manhattan, known for its academic excellence and research opportunities.\n",
            "2. New York University (NYU): A private research university located in Manhattan, known for its academic programs in business, law, and the arts.\n",
            "3. Cornell University: A private Ivy League university located in Ithaca, known for its academic programs in engineering, business, and agriculture.\n",
            "4. University at Buffalo: A public research university located in Buffalo, known for its academic programs in business, engineering, and the arts.\n",
            "\n",
            "**Economy:**\n",
            "\n",
            "1. Finance: New York is a global hub for finance, with Wall Street and the New York Stock Exchange (NYSE) located in Manhattan.\n",
            "2. Technology: New York is home to a thriving tech industry, with many startups and established companies based in the city.\n",
            "3. Healthcare: New York is a major center for healthcare, with many top-ranked hospitals and medical research institutions.\n",
            "4. Tourism: New York is a popular tourist destination, with over 65 million visitors per year.\n",
            "5. Manufacturing: New York has a diverse manufacturing sector, with industries ranging from food processing to aerospace.\n",
            "\n",
            "Overall, New York is a vibrant and diverse state with a rich history, culture, and economy. From its iconic cities to its natural beauty, there's something for everyone in the Empire State.\n"
          ]
        }
      ],
      "source": [
        "MODEL = \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"tell me about new york\"}],\n",
        ")\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89ad14fd",
      "metadata": {
        "id": "89ad14fd"
      },
      "source": [
        "Likewise, you can use the `th.get_tools()` function to display all of the Toolhouse tools you have installed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "d814c0bf",
      "metadata": {
        "id": "d814c0bf",
        "outputId": "a43004a6-3abc-44c9-eb80-f92a52aae184",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOOLS AVAILABLE:\n",
            "Name: search_x\n",
            "Type: function\n",
            "Description: This tool helps your LLM refine and target specific tweets (or posts) from X (formerly Twitter) database. By using advanced search operators, the LLM can compose precise search queries to locate the most relevant posts based on criteria like user, date, media type, and more.\n",
            "Name: image_generation_flux\n",
            "Type: function\n",
            "Description: This tools enables the LLM to generate images from a given description. The description comes in the format of a prompt. For example: `a photo of a cat, sitting on the couch, looking at the camera, hyper realistic, ultra high definition`. The image generated is returned to the LLM as a JSON containing a string which represents the image in an embeddable markdown format. Only return the markdown embed code to the user. For example: ![](https://example.com/image/url-string)\n"
          ]
        }
      ],
      "source": [
        "print('TOOLS AVAILABLE:')\n",
        "for tool in th.get_tools(\"together\"):\n",
        "    print(f\"Name: {tool['function']['name']}\")\n",
        "    print(f\"Type: {tool['type']}\")\n",
        "    print(f\"Description: {tool['function']['description']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b2b413",
      "metadata": {
        "id": "b5b2b413"
      },
      "source": [
        "For this demo, we will be using two tools from Toolhouse's store.\n",
        "- Search X\n",
        "- Image generation\n",
        "\n",
        "The first tool lets you search the X social network and the second tool lets you generate an image from a prompt and returns it to the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd565fc7",
      "metadata": {
        "id": "cd565fc7"
      },
      "source": [
        "### Configure Tool Calls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09bf2fec",
      "metadata": {
        "id": "09bf2fec"
      },
      "source": [
        "First we'll configure the user message to the LLM with the code we want run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "4b457ee7",
      "metadata": {
        "id": "4b457ee7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffbb5aa6-3ebe-4022-8eee-5d2f7f90ec9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'user', 'content': 'Find the last three messages from the account named @togethercompute on X/Twitter and summarize them in one sentence. Make it funny!'}]\n"
          ]
        }
      ],
      "source": [
        "# User message to the LLM\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"Find the last three messages from the account named @togethercompute on X/Twitter and summarize them in one sentence. Make it funny!\",\n",
        "  }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "396cabd1",
      "metadata": {
        "id": "396cabd1"
      },
      "source": [
        "We'll send this to our LLM hosted on TogetherAI. This allows you to skip having to code your own Search X parser since we are using the tool hosted and maintained by the Toolhouse team:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "08148e22",
      "metadata": {
        "id": "08148e22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5fc8d73-0278-454f-895b-a2ff03659a9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call_ynb9zloi1bp7012c28n36a01\n"
          ]
        }
      ],
      "source": [
        "# TogetherAI response with Toolhouse tools\n",
        "response = client.chat.completions.create(\n",
        "  model=MODEL,\n",
        "  messages=messages,\n",
        "  # Passes the tools to the model\n",
        "  tools=th.get_tools(\"together\"),\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.tool_calls[0].id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f279224f",
      "metadata": {
        "id": "f279224f"
      },
      "source": [
        "As you can see from the output above, the LLM properly identified that we'd like to invoke the 'search_x' tool\n",
        "\n",
        "You can see the arguments it generated by running the code below, it will break iut down by ID, type and function arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "8505b93d",
      "metadata": {
        "id": "8505b93d",
        "outputId": "8915eab0-6cbb-43ce-a8f0-3c61f4e7bddb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID: call_ynb9zloi1bp7012c28n36a01\n",
            "Type: function\n",
            "Function: Function(arguments='{\"query\":\"from:togethercompute\",\"max_results\":\"3\"}', name='search_x')\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tools_called = response.choices[0].message.tool_calls\n",
        "for tool_called in tools_called:\n",
        "    print(f\"ID: {tool_called.id}\")\n",
        "    print(f\"Type: {tool_called.type}\")\n",
        "    print(f\"Function: {tool_called.function}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e13ddcaf",
      "metadata": {
        "id": "e13ddcaf"
      },
      "source": [
        "### Execute Tool Call"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0a0978f",
      "metadata": {
        "id": "a0a0978f"
      },
      "source": [
        "Now that we learned how the LLM can generate arguments to pass to a function we'll have to **run the function with those arguments**.\n",
        "\n",
        "The tool gets executed via the `run_tools` command, with the parameters that were identified in the previous LLM call.\n",
        "\n",
        "\n",
        "After that we will get the result, and append it to the context, the `messages` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "355f0f50",
      "metadata": {
        "id": "355f0f50",
        "outputId": "8927611f-a206-4dc3-a7a5-e07cbad040a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='8e0b248b1bea8832-ATL', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_ynb9zloi1bp7012c28n36a01', function=Function(arguments='{\"query\":\"from:togethercompute\",\"max_results\":\"3\"}', name='search_x'), type='function', index=0)]), seed=14357862309395075000)], created=1731295187, model='meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=25, prompt_tokens=860, total_tokens=885, completion_tokens_details=None, prompt_tokens_details=None), prompt=[])\n"
          ]
        }
      ],
      "source": [
        "tool_run = th.run_tools(response)\n",
        "messages += tool_run"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what our messages list looks like:"
      ],
      "metadata": {
        "id": "aOTqRaFe7xMa"
      },
      "id": "aOTqRaFe7xMa"
    },
    {
      "cell_type": "code",
      "source": [
        "for message in tool_run:\n",
        "  print(f\"Role: {message['role']}\")\n",
        "  if 'tool_calls' in message:\n",
        "    print(f\"Tool Calls: {message['tool_calls']}\")\n",
        "  if 'tool_call_id' in message:\n",
        "    print(f\"Tool Call ID: {message['tool_call_id']}\")\n",
        "    print(f\"Content: {message['content']}\")\n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqeSLyYhJRMc",
        "outputId": "ba43561a-d012-4ee8-9e3a-dc347b4bfa35"
      },
      "id": "ZqeSLyYhJRMc",
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Role: assistant\n",
            "Tool Calls: [{'id': 'call_ynb9zloi1bp7012c28n36a01', 'function': {'arguments': '{\"query\":\"from:togethercompute\",\"max_results\":\"3\"}', 'name': 'search_x'}, 'type': 'function', 'index': 0}]\n",
            "Role: tool\n",
            "Tool Call ID: call_ynb9zloi1bp7012c28n36a01\n",
            "Content: <tweet>https://t.co/5OI0Gmyb7Y\n",
            "by @togethercompute at 2024-11-08T17:21:22.000Z\n",
            " Conversation ID: 1854937267552272638\n",
            "\n",
            "![github.com/togethercomput…](https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb\n",
            "<tweet>\n",
            "\n",
            "<tweet>New Cookbook: Summarizing and Evaluating Outputs with LLMs.\n",
            "\n",
            "In this notebook we:\n",
            "1. Generate summaries using Llama 3.1 70B\n",
            "2. Assess summary quality using BERTScore evaluated between original document chunks and the summary.\n",
            "\n",
            "Link Below: https://t.co/PWL3vPY1GU\n",
            "by @togethercompute at 2024-11-08T17:21:22.000Z\n",
            " Conversation ID: 1854937267552272638\n",
            "\n",
            "\n",
            "Photo: ![Image](https://pbs.twimg.com/media/Gb4PSoQWEAAov31.jpg)<tweet>\n",
            "\n",
            "<tweet>https://t.co/8ZBdRFnRPv\n",
            "by @togethercompute at 2024-11-07T18:45:47.000Z\n",
            " Conversation ID: 1854596121118597266\n",
            "\n",
            "![docs.together.ai/docs/how-to-im…](https://docs.together.ai/docs/how-to-implement-contextual-rag-from-anthropic\n",
            "<tweet>\n",
            "\n",
            "<tweet>New Guide: How to implement an open source version of Contextual RAG from Anthropic\n",
            "\n",
            "Contextual RAG is a great LLM based chunk augmentation technique. \n",
            "\n",
            "In this guide we break it down line by line and re-implement using open models.\n",
            "\n",
            "Read here: https://t.co/Zzd04MELyu\n",
            "by @togethercompute at 2024-11-07T18:45:46.000Z\n",
            " Conversation ID: 1854596121118597266\n",
            "\n",
            "\n",
            "Photo: ![Image](https://pbs.twimg.com/media/GbzZRM7XcBMpkfa.jpg)<tweet>\n",
            "\n",
            "<tweet>Recording is here!\n",
            "\n",
            "https://t.co/OAnmngO99e\n",
            "by @togethercompute at 2024-11-06T18:42:59.000Z\n",
            " Conversation ID: 1854203870433759574\n",
            "\n",
            "![youtube.com/watch?v=N1mXXg…](https://www.youtube.com/watch?v=N1mXXgw1GHM\n",
            "<tweet>\n",
            "\n",
            "<tweet>https://t.co/ENSQKrnh8n\n",
            "by @togethercompute at 2024-11-06T18:20:56.000Z\n",
            " Conversation ID: 1854227479436693915\n",
            "\n",
            "![docs.together.ai/docs/how-to-im…](https://docs.together.ai/docs/how-to-improve-search-with-rerankers\n",
            "<tweet>\n",
            "\n",
            "<tweet>New Guide: How to improve semantic search using rerankers.\n",
            "\n",
            "Rerankers are a great way to improve retrieval quality for your RAG pipeline, learn how you can use them!\n",
            "\n",
            "Read here: https://t.co/0SeXKoZZLG\n",
            "by @togethercompute at 2024-11-06T18:20:55.000Z\n",
            " Conversation ID: 1854227479436693915\n",
            "\n",
            "\n",
            "Photo: ![Image](https://pbs.twimg.com/media/GbuKBrYXcAQdsgV.jpg)<tweet>\n",
            "\n",
            "<tweet>🚀 Just 15 minutes away! 🚀\n",
            "\n",
            "Tune in as @nutlope breaks down how he built LlamaCoder—an open-source code generator with 400k users and 3k GitHub stars—in just one weekend! He'll share tips on scaling and creating your own AI apps with Next.js. Don’t miss out!\n",
            "\n",
            "RSVP here 👉 https://t.co/Fic9pVLKfu\n",
            "by @togethercompute at 2024-11-06T16:47:06.000Z\n",
            " Conversation ID: 1854203870433759574\n",
            "\n",
            "![lu.ma/euvemy0q](https://lu.ma/euvemy0q\n",
            "<tweet>\n",
            "\n",
            "<tweet>It's the little things ✨\n",
            "\n",
            "Our TypeScript library will now suggest popular models for all our different APIs (chat, completions, images, embeddings, rerank, ect...), directly in your editor! https://t.co/E82QvJFoTr\n",
            "by @togethercompute at 2024-11-05T21:22:57.000Z\n",
            " Conversation ID: 1853910899997786575\n",
            "\n",
            "\n",
            "(Video)[https://pbs.twimg.com/ext_tw_video_thumb/1853910474481410050/pu/img/U5n6-Y7hgBWUgIIo.jpg]\n",
            "Sources:\n",
            " [video/mp4](https://video.twimg.com/ext_tw_video/1853910474481410050/pu/vid/avc1/398x270/7s1g4zquB5qnkDLy.mp4?tag=12)\n",
            "[video/mp4](https://video.twimg.com/ext_tw_video/1853910474481410050/pu/vid/avc1/530x360/t87pwSvEb-SBkkap.mp4?tag=12)\n",
            "[video/mp4](https://video.twimg.com/ext_tw_video/1853910474481410050/pu/vid/avc1/1062x720/P4jsJ_mvleHsqmEK.mp4?tag=12)\n",
            "[application/x-mpegURL](https://video.twimg.com/ext_tw_video/1853910474481410050/pu/pl/sjXWyLwSAyr3rhis.m3u8?tag=12)<tweet>\n",
            "\n",
            "<tweet>https://t.co/EbRqad77EW\n",
            "by @togethercompute at 2024-11-05T17:13:29.000Z\n",
            " Conversation ID: 1853848117952815298\n",
            "\n",
            "![docs.together.ai/docs/building-…](https://docs.together.ai/docs/building-a-rag-workflow\n",
            "<tweet>\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84ceb341",
      "metadata": {
        "id": "84ceb341"
      },
      "source": [
        "We're now going to pass the whole message chain to the LLM. The LLM will generate a proper response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "84f24416",
      "metadata": {
        "id": "84f24416",
        "outputId": "f378a6c1-4157-4b0c-c97f-7360031c4479",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM RESPONSE: Here's a funny summary of the last three messages from @togethercompute: \"Together Compute is on a roll, releasing new guides on implementing Contextual RAG and improving semantic search, while also sharing a recording of a talk on building LlamaCoder, because who doesn't love a good AI app?\"\n"
          ]
        }
      ],
      "source": [
        "summary_response = client.chat.completions.create(\n",
        "  model=MODEL,\n",
        "  messages=messages\n",
        ")\n",
        "\n",
        "print('LLM RESPONSE:', summary_response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add this last response - a funny summary of three posts retrieved from X - to the history of messages.\n",
        "\n",
        "Then let's use another tool to generate an image."
      ],
      "metadata": {
        "id": "1sJuKocpNd0_"
      },
      "id": "1sJuKocpNd0_"
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "f3b5513b",
      "metadata": {
        "id": "f3b5513b",
        "outputId": "ddca8e5a-00d5-44db-a17a-8359067ce7b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM RESPONSE: [{'role': 'user', 'content': 'Find the last three messages from the account named @togethercompute on X/Twitter and summarize them in one sentence. Make it funny!'}, {'content': None, 'refusal': None, 'role': 'assistant', 'audio': None, 'tool_calls': [{'id': 'call_ynb9zloi1bp7012c28n36a01', 'function': {'arguments': '{\"query\":\"from:togethercompute\",\"max_results\":\"3\"}', 'name': 'search_x'}, 'type': 'function', 'index': 0}]}, {'role': 'tool', 'tool_call_id': 'call_ynb9zloi1bp7012c28n36a01', 'name': 'search_x', 'content': \"<tweet>https://t.co/5OI0Gmyb7Y\\nby @togethercompute at 2024-11-08T17:21:22.000Z\\n Conversation ID: 1854937267552272638\\n\\n![github.com/togethercomput…](https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb\\n<tweet>\\n\\n<tweet>New Cookbook: Summarizing and Evaluating Outputs with LLMs.\\n\\nIn this notebook we:\\n1. Generate summaries using Llama 3.1 70B\\n2. Assess summary quality using BERTScore evaluated between original document chunks and the summary.\\n\\nLink Below: https://t.co/PWL3vPY1GU\\nby @togethercompute at 2024-11-08T17:21:22.000Z\\n Conversation ID: 1854937267552272638\\n\\n\\nPhoto: ![Image](https://pbs.twimg.com/media/Gb4PSoQWEAAov31.jpg)<tweet>\\n\\n<tweet>https://t.co/8ZBdRFnRPv\\nby @togethercompute at 2024-11-07T18:45:47.000Z\\n Conversation ID: 1854596121118597266\\n\\n![docs.together.ai/docs/how-to-im…](https://docs.together.ai/docs/how-to-implement-contextual-rag-from-anthropic\\n<tweet>\\n\\n<tweet>New Guide: How to implement an open source version of Contextual RAG from Anthropic\\n\\nContextual RAG is a great LLM based chunk augmentation technique. \\n\\nIn this guide we break it down line by line and re-implement using open models.\\n\\nRead here: https://t.co/Zzd04MELyu\\nby @togethercompute at 2024-11-07T18:45:46.000Z\\n Conversation ID: 1854596121118597266\\n\\n\\nPhoto: ![Image](https://pbs.twimg.com/media/GbzZRM7XcBMpkfa.jpg)<tweet>\\n\\n<tweet>Recording is here!\\n\\nhttps://t.co/OAnmngO99e\\nby @togethercompute at 2024-11-06T18:42:59.000Z\\n Conversation ID: 1854203870433759574\\n\\n![youtube.com/watch?v=N1mXXg…](https://www.youtube.com/watch?v=N1mXXgw1GHM\\n<tweet>\\n\\n<tweet>https://t.co/ENSQKrnh8n\\nby @togethercompute at 2024-11-06T18:20:56.000Z\\n Conversation ID: 1854227479436693915\\n\\n![docs.together.ai/docs/how-to-im…](https://docs.together.ai/docs/how-to-improve-search-with-rerankers\\n<tweet>\\n\\n<tweet>New Guide: How to improve semantic search using rerankers.\\n\\nRerankers are a great way to improve retrieval quality for your RAG pipeline, learn how you can use them!\\n\\nRead here: https://t.co/0SeXKoZZLG\\nby @togethercompute at 2024-11-06T18:20:55.000Z\\n Conversation ID: 1854227479436693915\\n\\n\\nPhoto: ![Image](https://pbs.twimg.com/media/GbuKBrYXcAQdsgV.jpg)<tweet>\\n\\n<tweet>🚀 Just 15 minutes away! 🚀\\n\\nTune in as @nutlope breaks down how he built LlamaCoder—an open-source code generator with 400k users and 3k GitHub stars—in just one weekend! He'll share tips on scaling and creating your own AI apps with Next.js. Don’t miss out!\\n\\nRSVP here 👉 https://t.co/Fic9pVLKfu\\nby @togethercompute at 2024-11-06T16:47:06.000Z\\n Conversation ID: 1854203870433759574\\n\\n![lu.ma/euvemy0q](https://lu.ma/euvemy0q\\n<tweet>\\n\\n<tweet>It's the little things ✨\\n\\nOur TypeScript library will now suggest popular models for all our different APIs (chat, completions, images, embeddings, rerank, ect...), directly in your editor! https://t.co/E82QvJFoTr\\nby @togethercompute at 2024-11-05T21:22:57.000Z\\n Conversation ID: 1853910899997786575\\n\\n\\n(Video)[https://pbs.twimg.com/ext_tw_video_thumb/1853910474481410050/pu/img/U5n6-Y7hgBWUgIIo.jpg]\\nSources:\\n [video/mp4](https://video.twimg.com/ext_tw_video/1853910474481410050/pu/vid/avc1/398x270/7s1g4zquB5qnkDLy.mp4?tag=12)\\n[video/mp4](https://video.twimg.com/ext_tw_video/1853910474481410050/pu/vid/avc1/530x360/t87pwSvEb-SBkkap.mp4?tag=12)\\n[video/mp4](https://video.twimg.com/ext_tw_video/1853910474481410050/pu/vid/avc1/1062x720/P4jsJ_mvleHsqmEK.mp4?tag=12)\\n[application/x-mpegURL](https://video.twimg.com/ext_tw_video/1853910474481410050/pu/pl/sjXWyLwSAyr3rhis.m3u8?tag=12)<tweet>\\n\\n<tweet>https://t.co/EbRqad77EW\\nby @togethercompute at 2024-11-05T17:13:29.000Z\\n Conversation ID: 1853848117952815298\\n\\n![docs.together.ai/docs/building-…](https://docs.together.ai/docs/building-a-rag-workflow\\n<tweet>\\n\"}, {'role': 'assistant', 'content': 'Here\\'s a funny summary of the last three messages from @togethercompute: \"Together Compute is on a roll, releasing new guides on implementing Contextual RAG and improving semantic search, while also sharing a recording of a talk on building LlamaCoder, because who doesn\\'t love a good AI app?\"'}, {'role': 'user', 'content': 'Generate an image from the funny summary'}, {'content': None, 'refusal': None, 'role': 'assistant', 'audio': None, 'tool_calls': [{'id': 'call_xvdk0n7go4uh3zm5i8aznxft', 'function': {'arguments': '{\"prompt\":\"a funny image of a computer on a roll, releasing new guides and sharing a recording of a talk on building LlamaCoder\",\"width\":\"1024\",\"height\":\"1024\",\"steps\":\"50\"}', 'name': 'image_generation_flux'}, 'type': 'function', 'index': 0}]}, {'role': 'tool', 'tool_call_id': 'call_xvdk0n7go4uh3zm5i8aznxft', 'name': 'image_generation_flux', 'content': '{\"result\":\"![](https://img.toolhouse.ai/H2BI1f.jpg)\"}'}]\n"
          ]
        }
      ],
      "source": [
        "new_messages = messages + [\n",
        "  {\n",
        "    \"role\": \"assistant\",\n",
        "    \"content\": summary_response.choices[0].message.content,\n",
        "  }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we're going to give a new command to the LLM - generate us an image from this funny caption using the tool provided by Toolhouse.\n",
        "\n",
        "This tool uses Flux to take a prompt and generates an image, then hosts it  to storage on the img.toolhouse.ai subdomain."
      ],
      "metadata": {
        "id": "T9K6hrDkbbIP"
      },
      "id": "T9K6hrDkbbIP"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "generate_image_request = new_messages + [\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"Generate an image from the funny summary\"\n",
        "  }\n",
        "]\n",
        "\n",
        "# Let's ask the LLM to generate the image\n",
        "image_response = client.chat.completions.create(\n",
        "  model=MODEL,\n",
        "  messages=generate_image_request,\n",
        "  tools=th.get_tools(),\n",
        ")\n",
        "tool_run = th.run_tools(image_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "rQtHdIjObpZA",
        "outputId": "a152e4ae-6acb-439d-a112-e3f4999dfa0c"
      },
      "id": "rQtHdIjObpZA",
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-144-c0803c1c945b>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m image_response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate_image_request\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    813\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    814\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    816\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         )\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    955\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             response = self._client.send(\n\u001b[0m\u001b[1;32m    991\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    924\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    927\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    955\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    989\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m         )\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    197\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    114\u001b[0m                 trace.return_value = (\n\u001b[1;32m    115\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    225\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1286\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the LLM generated an image and stored it, we just want it to let the user know that the image can be viewed. So we're going to yet again ask the LLM to take the output of the function call and generate an image."
      ],
      "metadata": {
        "id": "-9RI1IWZbX4I"
      },
      "id": "-9RI1IWZbX4I"
    },
    {
      "cell_type": "code",
      "source": [
        "image_messages = generate_image_request + tool_run\n",
        "\n",
        "last_response = client.chat.completions.create(\n",
        "  model=MODEL,\n",
        "  messages=image_messages,\n",
        "  tools=th.get_tools(),\n",
        ")\n",
        "\n",
        "print('LLM RESPONSE:', last_response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gjk6rNvfXanT",
        "outputId": "20b8e753-f9ab-4ad5-cd5c-52e830866bc8"
      },
      "id": "Gjk6rNvfXanT",
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM RESPONSE: Here is the image from the funny summary: ![](https://img.toolhouse.ai/w0CBb3.jpg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And there you have it, if you have managed to run all the cells in the correct order you should see a message with a valid URL. If you click it you will be able to see the generated image!"
      ],
      "metadata": {
        "id": "TwDJcIG_cN46"
      },
      "id": "TwDJcIG_cN46"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}