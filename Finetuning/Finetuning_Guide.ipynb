{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéõÔ∏è Fine-tuning Guide \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) offer powerful general capabilities, but often require **fine-tuning** to excel at specific tasks or understand domain-specific language. Fine-tuning adapts a trained model to a smaller, targeted dataset, enhancing its performance for your unique needs.\n",
    "\n",
    "This notebook provides a step-by-step guide to fine-tuning models using the Together AI platform. We will walk through the entire process, from preparing your data to evaluating your fine-tuned model.\n",
    "\n",
    "We will cover:\n",
    "\n",
    "1.  **Dataset Preparation:** Loading a standard dataset, transforming it into the required format for supervised fine-tuning on Together AI, and uploading your formatted dataset to Together AI Files.\n",
    "2.  **Fine-tuning Job Launch:** Configuring and initiating a fine-tuning job using the Together AI API.\n",
    "3.  **Job Monitoring:** Checking the status and progress of your fine-tuning job.\n",
    "4.  **Inference:** Using your newly finetuned model via the Together AI API for predictions.\n",
    "5.  **Evaluation:** Comparing the performance of the finetuned model against the base model on a test set.\n",
    "\n",
    "By following this guide, you'll gain practical experience in creating specialized LLMs tailored to your specific requirements using Together AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "---\n",
    "First, install the necessary Python libraries. We need:\n",
    "- `together`: The official Together AI Python client for interacting with the API (fine-tuning, inference, files, etc.).\n",
    "- `datasets`: A library from Hugging Face for easily downloading and manipulating datasets.\n",
    "- `transformers`: Although we won't be training locally, this can be useful for running evals and other utilities if needed.\n",
    "- `tqdm`: To enable interactive elements like progress bars within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU together datasets transformers tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation\n",
    "---\n",
    "Fine-tuning requires data formatted in a specific way. We'll use the a conversational dataset as an example - here the goal of the fine-tuning is to improve the model on multi-turn conversations. \n",
    "\n",
    "First we need to transform this dataset into the chat format expected by Together AI for supervised fine-tuning.\n",
    "\n",
    "The required format is a JSON object per line, where each object contains a list of conversation turns under the `\"messages\"` key.\n",
    "\n",
    "Each message must have a `\"role\"` (`system`, `user`, or `assistant`) and `\"content\"`.\n",
    "\n",
    "Conversation Data Example:\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "              {\"role\": \"user\", \"content\": \"Hello!\"}, \n",
    "              {\"role\": \"assistant\", \"content\": \"Hi! How can I help you?\"}]}\n",
    "```\n",
    "\n",
    "üîóDepending on what type of fine-tuning you want to perform you can also pass in [instruction data](https://docs.together.ai/docs/fine-tuning-data-preparation#instruction-data), [preference data](https://docs.together.ai/docs/fine-tuning-data-preparation#preference-data) or even [simple text data](https://docs.together.ai/docs/fine-tuning-data-preparation#generic-text-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Raw Dataset\n",
    "We use the `datasets` library to download the CoQA dataset from the Hugging Face Hub.\n",
    "\n",
    "Let's examine the structure of the raw dataset. CoQA provides a story, a series of questions related to the story, and corresponding answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "coqa_dataset = load_dataset(\"stanfordnlp/coqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>story</th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>[When was the Vat formally opened?, what is th...</td>\n",
       "      <td>{'input_text': ['It was formally established i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cnn</td>\n",
       "      <td>New York (CNN) -- More than 80 Michael Jackson...</td>\n",
       "      <td>[Where was the Auction held?, How much did the...</td>\n",
       "      <td>{'input_text': ['Hard Rock Cafe', '$2 million....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gutenberg</td>\n",
       "      <td>CHAPTER VII. THE DAUGHTER OF WITHERSTEEN \\n\\n\"...</td>\n",
       "      <td>[What did Venters call Lassiter?, Who asked La...</td>\n",
       "      <td>{'input_text': ['gun-man', 'Jane', 'Yes', 'to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnn</td>\n",
       "      <td>(CNN) -- The longest-running holiday special s...</td>\n",
       "      <td>[Who is Rudolph's father?, Why does Rudolph ru...</td>\n",
       "      <td>{'input_text': ['Donner', 'he felt like an out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gutenberg</td>\n",
       "      <td>CHAPTER XXIV. THE INTERRUPTED MASS \\n\\nThe mor...</td>\n",
       "      <td>[Who arrived at the church?, Who was followed ...</td>\n",
       "      <td>{'input_text': ['the garrison first', 'Fra. Do...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source                                              story  \\\n",
       "0  wikipedia  The Vatican Apostolic Library (), more commonl...   \n",
       "1        cnn  New York (CNN) -- More than 80 Michael Jackson...   \n",
       "2  gutenberg  CHAPTER VII. THE DAUGHTER OF WITHERSTEEN \\n\\n\"...   \n",
       "3        cnn  (CNN) -- The longest-running holiday special s...   \n",
       "4  gutenberg  CHAPTER XXIV. THE INTERRUPTED MASS \\n\\nThe mor...   \n",
       "\n",
       "                                           questions  \\\n",
       "0  [When was the Vat formally opened?, what is th...   \n",
       "1  [Where was the Auction held?, How much did the...   \n",
       "2  [What did Venters call Lassiter?, Who asked La...   \n",
       "3  [Who is Rudolph's father?, Why does Rudolph ru...   \n",
       "4  [Who arrived at the church?, Who was followed ...   \n",
       "\n",
       "                                             answers  \n",
       "0  {'input_text': ['It was formally established i...  \n",
       "1  {'input_text': ['Hard Rock Cafe', '$2 million....  \n",
       "2  {'input_text': ['gun-man', 'Jane', 'Yes', 'to ...  \n",
       "3  {'input_text': ['Donner', 'he felt like an out...  \n",
       "4  {'input_text': ['the garrison first', 'Fra. Do...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coqa_dataset[\"train\"].to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data to Chat Format\n",
    "\n",
    "Now, we need to convert each row of the CoQA dataset into the required chat format (`[{'role': ..., 'content': ...}, ...]`).\n",
    "\n",
    "We'll create a function `map_coqa_to_chat_format` that takes a row from the dataset and structures it as a conversation:\n",
    "1.  A `system` message containing the story (context).\n",
    "2.  Alternating `user` (question) and `assistant` (answer) messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the system prompt,if present, must always be at the beginning\n",
    "system_prompt = \"Read the story and extract answers for the questions.\\nStory: {}\"\n",
    "\n",
    "def map_fields(row):\n",
    "    \"\"\"    \n",
    "    Maps the fields from a row of data to a structured format for conversation.\n",
    "    Args:\n",
    "        row (dict): A dictionary containing the keys \"story\", \"questions\", and \"answers\".\n",
    "            - \"story\" (str): The story content to be used in the system prompt.\n",
    "            - \"questions\" (list of str): A list of questions from the user.\n",
    "            - \"answers\" (dict): A dictionary containing the key \"input_text\" which is a list of answers from the assistant.\n",
    "    Returns:\n",
    "        dict: A dictionary with a single key \"messages\" which is a list of message dictionaries.\n",
    "            Each message dictionary contains:\n",
    "            - \"role\" (str): The role of the message sender, either \"system\", \"user\", or \"assistant\".\n",
    "            - \"content\" (str): The content of the message.    \n",
    "    \"\"\"\n",
    "    # create system prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt.format(row[\"story\"])}]\n",
    "    \n",
    "    # add user and assistant messages\n",
    "    for q, a in zip(row[\"questions\"], row[\"answers\"][\"input_text\"]):\n",
    "        messages.append({\"role\": \"user\", \"content\": q})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": a})\n",
    "    \n",
    "    return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply this transformation function to the entire dataset using the `.map()` method. We also remove the original columns as they are no longer needed after transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data using the mapping function\n",
    "\n",
    "train_messages = coqa_dataset[\"train\"].map(map_fields, remove_columns=coqa_dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the structure of our transformed dataset. It should now only contain the `messages` column.\n",
    "\n",
    "Here's an example of a single processed data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 7199\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the dataset out to a `json` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5155661f6e154ded8ee08e8980d78722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "23777505"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_messages.to_json(\"coqa_prepared_train.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data to Together AI\n",
    "\n",
    "Now that we have our formatted `coqa_prepared_train.jsonl` files, we need to check if they meet the format specification and then upload them to Together AI. Fine-tuning jobs read data directly from your uploaded files.\n",
    "\n",
    "We use the `check_file` function to check the file and `files.upload()` method. This returns information about the uploaded file, including its ID, which we'll need later to start the fine-tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup Together AI client\n",
    "from together import Together\n",
    "import os\n",
    "import json\n",
    "\n",
    "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\") # needed for logging fine-tuning to wandb\n",
    "\n",
    "\n",
    "client = Together(api_key=TOGETHER_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"is_check_passed\": true,\n",
      "  \"message\": \"Checks passed\",\n",
      "  \"found\": true,\n",
      "  \"file_size\": 23777505,\n",
      "  \"utf8\": true,\n",
      "  \"line_type\": true,\n",
      "  \"text_field\": true,\n",
      "  \"key_value\": true,\n",
      "  \"has_min_samples\": true,\n",
      "  \"num_samples\": 7199,\n",
      "  \"load_json\": true,\n",
      "  \"filetype\": \"jsonl\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## We're going to check to see that the file is in the right format before we finetune\n",
    "from together.utils import check_file\n",
    "\n",
    "sft_report = check_file(\"coqa_prepared_train.jsonl\")\n",
    "print(json.dumps(sft_report, indent=2))\n",
    "\n",
    "assert sft_report[\"is_check_passed\"] == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading file coqa_prepared_train.jsonl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23.8M/23.8M [00:01<00:00, 15.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file response: file-9554964d-5711-419a-bcc2-c4edaaa07ee3\n"
     ]
    }
   ],
   "source": [
    "## Upload the data to Together\n",
    "\n",
    "train_file_resp = client.files.upload(\"coqa_prepared_train.jsonl\", check=True)\n",
    "print(f\"Train file response: {train_file_resp.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Launch Fine-tuning Job\n",
    "---\n",
    "With our data uploaded, we can now launch the fine-tuning job using `together.Finetune.create()`.\n",
    "\n",
    "Key parameters:\n",
    "- `model`: The base model you want to finetune (e.g., `'togethercomputer/llama-2-7b-chat'`). Choose from the models available for fine-tuning on Together AI.\n",
    "- `training_file`: The ID of your uploaded training JSONL file.\n",
    "- `validation_file`: The ID of your uploaded validation JSONL file (optional, but highly recommended for monitoring).\n",
    "- `suffix`: A custom string added to the base model name to create your unique finetuned model name (e.g., `my-coqa-ft`). Keep it short and descriptive.\n",
    "- `n_epochs`: The number of times the model will see the entire training dataset.\n",
    "- `n_checkpoints`: Number of checkpoints to save during training (useful for resuming or selecting the best model). Set to 1 if you only need the final model.\n",
    "- `learning_rate`: Controls how much the model weights are updated during training. Needs tuning.\n",
    "- `batch_size`: Number of training examples processed in one iteration. Depends on model size and available resources.\n",
    "\n",
    "üîó For an exhaustive list of all the available fine-tuning parameters refer to the [Together AI Fine-tuning API Reference](https://docs.together.ai/reference/post_fine-tunes) docs.\n",
    "\n",
    "üîó For a list of all the models you can fine-tune on the Together AI platform see [docs here](https://docs.together.ai/docs/fine-tuning-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This fine-tuning job should take ~10-15 minutes to complete\n",
    "\n",
    "ft_resp = client.fine_tuning.create(\n",
    "    training_file = \"file-19c6ef51-b734-4f3c-bc17-62fbad2bd0d0\",\n",
    "    model = 'meta-llama/Meta-Llama-3.1-8B-Instruct-Reference',\n",
    "    train_on_inputs= \"auto\",\n",
    "    n_epochs = 3,\n",
    "    n_checkpoints = 1,\n",
    "    wandb_api_key = WANDB_API_KEY,\n",
    "    lora = True,\n",
    "    warmup_ratio=0,\n",
    "    learning_rate = 1e-5,\n",
    "    suffix = 'test1_8b',\n",
    ")\n",
    "\n",
    "print(ft_resp.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monitor Fine-tuning Job\n",
    "---\n",
    "Fine-tuning can take time depending on the model size, dataset size, and hyperparameters. You can monitor and alter the job's progress using the following methods:\n",
    "\n",
    "- List all jobs: `client.fine_tuning.list()`\n",
    "- Status of a Job:`client.fine_tuning.retrieve(id=ft_resp.id)`\n",
    "- List all events for a Job: `client.fine_tuning.list_events(id=ft_resp.id)`: Retrieves logs and events generated during the job\n",
    "- Cancel job: `client.fine_tuning.cancel(id=ft_resp.id)`\n",
    "- Download model after done: `client.fine_tuning.download(id=ft_resp.id)`\n",
    "\n",
    "Once the job is complete (`status == 'completed'`), the response from `retrieve` will contain the name of your newly created finetuned model. It follows the pattern: `<your-account>/<base-model-name>:<suffix>:<job-id>`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinetuneJobStatus.STATUS_COMPLETED\n"
     ]
    }
   ],
   "source": [
    "# Check status of the job\n",
    "resp = client.fine_tuning.retrieve(ft_resp.id)\n",
    "print(resp.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tune request created\n",
      "Job started at Wed Apr  9 19:48:05 UTC 2025\n",
      "Model data downloaded for togethercomputer/Meta-Llama-3.1-8B-Instruct-Reference__TOG__FT at Wed Apr  9 19:48:07 UTC 2025\n",
      "Data downloaded for togethercomputer/Meta-Llama-3.1-8B-Instruct-Reference__TOG__FT at $2025-04-09T19:48:14.918488\n",
      "WandB run initialized.\n",
      "Training started for model togethercomputer/Meta-Llama-3.1-8B-Instruct-Reference__TOG__FT\n",
      "Epoch completed, at step 24\n",
      "Epoch completed, at step 48\n",
      "Epoch completed, at step 72\n",
      "Training completed for togethercomputer/Meta-Llama-3.1-8B-Instruct-Reference__TOG__FT at Wed Apr  9 20:02:24 UTC 2025\n",
      "Uploading adapter model\n",
      "Compressing output model\n",
      "Model compression complete\n",
      "Uploading output model\n",
      "Model upload complete\n",
      "Job finished at Wed Apr  9 20:06:33 UTC 2025\n"
     ]
    }
   ],
   "source": [
    "# this loop will print the logs of the job thus far\n",
    "for event in client.fine_tuning.list_events(id=ft_resp.id).data:\n",
    "    print(event.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîó You can also navigate to the WandB page linked in your [fine-tuning dashboard](https://api.together.ai/fine-tuning) to see the fine-tuning related loss curves and more.\n",
    "\n",
    "<img src=\"../images/FT_run.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference with Fine-tuned Model\n",
    "---\n",
    "### Option 1: Serverless LoRA Inference\n",
    "\n",
    "Now, let's use our finetuned model! We can call it just like any other model on the Together AI platform, by providing the unique fine-tuned model `output_name` we retrieved from our fine-tuning job earlier.\n",
    "\n",
    "üîó See the list of all models that support [LoRA Inference](https://docs.together.ai/docs/lora-inference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model output_name: zainhas/Meta-Llama-3.1-8B-Instruct-Reference-test1_8b-e5a0fb5d\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fine-tuned model output_name: {ft_resp.output_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# The first time you run this it'll take longer to load the adapter weights for the first time\n",
    "\n",
    "finetuned_model = ft_resp.output_name #this is the name of the finetuned model\n",
    "\n",
    "user_prompt = \"What is the capital of France?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = finetuned_model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=124,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also prompt the model in our playground, if it support serverless LoRA Inference, by going to your [your models dashboard](https://api.together.xyz/models) and clicking \"OPEN IN PLAYGROUND\".\n",
    "\n",
    "<img src=\"../images/open_in_playground.png\" alt=\"Open in Playground button\" width=\"900\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Deploy Dedicated Endpoint\n",
    "\n",
    "Another way to run your fine-tuned model is to deploy it on a custom dedicated endpoint. \n",
    "\n",
    "Once your fine-tuning job completes, you should see your new model in [your models dashboard](https://api.together.xyz/models). You can click the \"+ CREATE DEDICATED ENDPOINT\" button to deploy the selected model to a DE.\n",
    "\n",
    "<img src=\"../images/create_DE.png\" width=\"900\">\n",
    "\n",
    "You can then select the hardware configuration for your dedicated endpoint including the min and max replicas which increases the maximum QPS the deployment can support.\n",
    "\n",
    "<img src=\"../images/deploy_DE.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also deploy the model to a DE programmatically using the `Endpoints` API via the SDK:\n",
    "\n",
    "```python\n",
    "response = client.endpoints.create(\n",
    "    display_name=\"Fine-tuned Meta Llama 3.1 8B Instruct 04-09-25\",\n",
    "    model=\"zainhas/Meta-Llama-3.1-8B-Instruct-Reference-test1_8b-e5a0fb5d\",\n",
    "    hardware=\"4x_nvidia_h100_80gb_sxm\",\n",
    "    autoscaling={\n",
    "        min_replicas: 1,\n",
    "        max_replicas: 1\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)\n",
    "```\n",
    "‚ö†Ô∏è If you run this code it will deploy a dedicated endpoint for you. For an detailed documentation around how to deploy, delete and modify endpoints see the [Endpoints API Reference](https://docs.together.ai/reference/createendpoint)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once deployed you'll be able to see the model details under your [Endpoints Dashboard](https://api.together.ai/endpoints):\n",
    "\n",
    "\n",
    "<img src=\"../images/deployed_DE.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# You can now query the endpoint as follows\n",
    "response = client.chat.completions.create(\n",
    "    model=\"zainhas/Meta-Llama-3.1-8B-Instruct-Reference-test1_8b-e5a0fb5d-ded38e09\",\n",
    "    messages=[{\"role\": \"user\", \n",
    "               \"content\": \"What is the capital of France?\"\n",
    "               }\n",
    "              ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "---\n",
    "To assess the impact of fine-tuning, we can compare the responses of our finetuned model with the original base model on the same prompt in out test set.\n",
    "\n",
    "This provides a way to measure improvements, after fine-tuning, to the model's behavior for our specific task (conversational QA based on a story)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import transformers.data.metrics.squad_metrics as squad_metrics\n",
    "\n",
    "base_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "finetuned_model = ft_resp.output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll load in 50 conversations from the CoQA validation set for evaluation\n",
    "coqa_dataset_validation = load_dataset(\"stanfordnlp/coqa\", split=\"validation[:50]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'story', 'questions', 'answers'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coqa_dataset_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the code below to generate answers from the baseline and fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function is used to generate model answers on the CoQA validation set from the untuned reference and fine-tuned models\n",
    "\n",
    "def get_model_answers(model_name):\n",
    "    \"\"\"\n",
    "    Generate model answers for a given model name using a dataset of questions and answers.\n",
    "    Args:\n",
    "        model_name (str): The name of the model to use for generating answers.\n",
    "    Returns:\n",
    "        list: A list of lists, where each inner list contains the answers generated by the model for the corresponding set of questions in the dataset.\n",
    "    The function performs the following steps:\n",
    "    1. Initializes an empty list to store the model answers.\n",
    "    2. Defines an inner function `get_answers` that takes a data dictionary and generates answers for the questions in the data.\n",
    "    3. Uses a thread pool to parallelize the process of generating answers for each entry in the validation dataset.\n",
    "    4. Appends the generated answers to the `model_answers` list.\n",
    "    5. Returns the `model_answers` list.\n",
    "    Note:\n",
    "        - The `client` variable is assumed to be defined elsewhere in the code.\n",
    "        - The `coqa_dataset` variable is assumed to contain the dataset with a \"validation\" key.\n",
    "    \"\"\"\n",
    "\n",
    "    model_answers = []\n",
    "    system_prompt = \"Read the story and extract answers for the questions.\\nStory: {}\"\n",
    "    \n",
    "    def get_answers(data):\n",
    "        answers = []\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt.format(data[\"story\"]),\n",
    "            }\n",
    "        ]\n",
    "        for q, true_answer in zip(data[\"questions\"], data[\"answers\"][\"input_text\"]):\n",
    "            try:\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": q\n",
    "                    }\n",
    "                )\n",
    "                response = client.chat.completions.create(\n",
    "                    messages=messages,\n",
    "                    model=model_name,\n",
    "                    max_tokens=64\n",
    "                )\n",
    "                answer = response.choices[0].message.content\n",
    "                answers.append(answer)\n",
    "            except Exception:\n",
    "                answers.append(\"Invalid Response\")\n",
    "        return answers\n",
    "\n",
    "    # We'll use 8 threads to generate answers faster in parallel\n",
    "    with ThreadPool(8) as pool:\n",
    "        for answers in tqdm(pool.imap(get_answers, coqa_dataset_validation), total=len(coqa_dataset_validation)):\n",
    "            model_answers.append(answers)\n",
    "\n",
    "    return model_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the function below to calculate the exact match and F1 score metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function will be used to evaluate predicted answers using the Exact Match (EM) and F1 metrics\n",
    "def get_metrics(pred_answers):\n",
    "    \"\"\"\n",
    "    Calculate the Exact Match (EM) and F1 metrics for predicted answers.\n",
    "    Args:\n",
    "        pred_answers (list): A list of predicted answers. Each element in the list is a list of predicted answers for a single question.\n",
    "    Returns:\n",
    "        tuple: A tuple containing two elements:\n",
    "            - em_score (float): The average Exact Match score across all predictions.\n",
    "            - f1_score (float): The average F1 score across all predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    em_metrics = []\n",
    "    f1_metrics = []\n",
    "\n",
    "    for pred, data in tqdm(zip(pred_answers, coqa_dataset_validation), total=len(pred_answers)):\n",
    "        for pred_answer, true_answer in zip(pred, data[\"answers\"][\"input_text\"]):\n",
    "            em_metrics.append(squad_metrics.compute_exact(true_answer, pred_answer))\n",
    "            f1_metrics.append(squad_metrics.compute_f1(true_answer, pred_answer))\n",
    "\n",
    "    return sum(em_metrics) / len(em_metrics), sum(f1_metrics) / len(f1_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f4e0b042ee414c846cfdb4bba839b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Base Model answers\n",
    "answers = get_model_answers(\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab94013cd5244f1916b9b8a315418d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Baseline, \n",
      "\n",
      "EM: 0.0175, F1: 0.18467257739023207\n"
     ]
    }
   ],
   "source": [
    "# calculate the EM and F1 metrics for baseline model\n",
    "em_metric, f1_metric = get_metrics(answers)\n",
    "print(f\"Model: Baseline, \\n\\nEM: {em_metric}, F1: {f1_metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zainhas/Meta-Llama-3.1-8B-Instruct-Reference-test1_8b-e5a0fb5d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c82df7ee31d4f3e885be89e3d6127f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"zainhas/Meta-Llama-3.1-8B-Instruct-Reference-test1_8b-e5a0fb5d\"\n",
    "\n",
    "print(model_name)\n",
    "answers = get_model_answers(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c0644d48ef4638aa8be1dc08af3a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: zainhas/Meta-Llama-3.1-8B-Instruct-Reference-test1_8b-e5a0fb5d, \n",
      "\n",
      "EM: 0.31, F1: 0.41019649357988347\n"
     ]
    }
   ],
   "source": [
    "em_metric, f1_metric = get_metrics(answers)\n",
    "print(f\"Model: {model_name}, \\n\\nEM: {em_metric}, F1: {f1_metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Llama 3.1 8B | EM | F1|\n",
    "|---|---|---|\n",
    "| Original | 0.01 | 0.18 |\n",
    "| Fine-tuned | 0.31 | 0.41 |\n",
    "\n",
    "We can see that the fine-tuned model performs twice as well on the test set when measuring the F1 score.\n",
    "\n",
    "For a more detailed guide on Fine-tuning follow our [docs here](https://docs.together.ai/docs/finetuning)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
