{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Contextual RAG Workflow with Together.AI on Union\n",
    "\n",
    "This notebook walks you through building a Contextual RAG (Retrieval-Augmented Generation) workflow on Union using Together's embedding, reranker, and chat models. It ties together web scraping, embedding generation, and serving into one cohesive application.\n",
    "\n",
    "[Union](https://www.union.ai/) is a production-grade AI orchestrator that unifies data, models, and compute workflows into a single, intuitive platform. It makes scalable, enterprise-ready AI accessible to all teams.\n",
    "\n",
    "In this notebook, we take the [existing Contextual RAG Together app](https://docs.together.ai/docs/how-to-implement-contextual-rag-from-anthropic) and make it \"production-grade\" with Union — ready for enterprise deployment.\n",
    "\n",
    "## Workflow overview\n",
    "\n",
    "The workflow follows these steps:\n",
    "\n",
    "1. Fetches all links to Paul Graham's essays.\n",
    "2. Scrapes web content to retrieve the full text of the essays.\n",
    "3. Splits the text into smaller chunks for processing.\n",
    "4. Appends context from the relevant essay to each chunk.\n",
    "5. Generates embeddings and stores them in a hosted vector database.\n",
    "6. Creates a keyword index for efficient retrieval.\n",
    "7. Serves a FastAPI app to expose the RAG functionality.\n",
    "8. Provides a Gradio app, using the FastAPI endpoint, for an easy-to-use RAG interface.\n",
    "\n",
    "Later in the notebook, we’ll dive into the Union features that simplify this entire process.\n",
    "\n",
    "## Execution approach\n",
    "\n",
    "This workflow is designed for local execution first, allowing you to test and validate it before deploying and scaling it on a Union cluster. This staged approach ensures smooth transitions from development to production.\n",
    "\n",
    "Before running the workflow, make sure to install `union`:\n",
    "\n",
    "```\n",
    "pip install union\n",
    "```\n",
    "\n",
    "### Local execution\n",
    "\n",
    "First, we import the required dependencies to ensure the workflow runs smoothly. Next, we define an actor environment, as the workflow relies on actor tasks throughout the process.\n",
    "\n",
    "[Union Actors](https://docs.union.ai/serverless/user-guide/core-concepts/actors) let us reuse a container and its environment across tasks, avoiding the overhead of starting a new container for each task. In this workflow, we define a single actor and reuse it consistently since the underlying components don’t require independent scaling or separate environments.\n",
    "\n",
    "Within the actor environment, we specify the `ImageSpec`, which defines the container image that tasks in the workflow will use. With Union, every task runs in its own dedicated container, requiring a container image. Instead of manually creating a Dockerfile, we define the image specification in Python. When run on Union Serverless, the container image is built remotely, simplifying the setup.\n",
    "\n",
    "We also configure the actor’s replica count to 10, meaning 10 workers are provisioned to handle tasks, allowing up to 10 tasks to run in parallel, provided sufficient resources. The TTL (time to live) is set to 120 seconds, ensuring the actor remains active for this period when no tasks are being processed.\n",
    "\n",
    "Finally, we create a Pydantic `BaseModel` named `Document` to capture metadata for each document used by the RAG app. This model ensures consistent data structuring and smooth integration throughout the workflow.\n",
    "\n",
    "NOTE: Add your Together.ai API key (`TOGETHER_API_KEY`) and the name of your registry (`REGISTRY`) to the `.env` file before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Annotated, Optional\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import flytekit as fl\n",
    "import numpy as np\n",
    "import requests\n",
    "import union\n",
    "from flytekit.core.artifact import Artifact\n",
    "from flytekit.exceptions.base import FlyteRecoverableException\n",
    "from flytekit.types.directory import FlyteDirectory\n",
    "from flytekit.types.file import FlyteFile\n",
    "from pydantic import BaseModel\n",
    "from union.actor import ActorEnvironment\n",
    "\n",
    "actor = ActorEnvironment(\n",
    "    name=\"contextual-rag\",\n",
    "    replica_count=10,\n",
    "    ttl_seconds=120,\n",
    "    container_image=union.ImageSpec(\n",
    "        name=\"contextual-rag\",\n",
    "        packages=[\n",
    "            \"together==1.3.10\",\n",
    "            \"beautifulsoup4==4.12.3\",\n",
    "            \"bm25s==0.2.5\",\n",
    "            \"pydantic>2\",\n",
    "            \"pymilvus>=2.5.4\",\n",
    "            \"union>=0.1.139\",\n",
    "            \"git+https://github.com/flyteorg/flytekit.git@4208a641debb0334c49c9331bcc4d98ed5c45d12\", # TODO: Need a new flytekit release\n",
    "        ],\n",
    "        apt_packages=[\"git\"],\n",
    "    ),\n",
    "    secret_requests=[\n",
    "        fl.Secret(\n",
    "            key=\"together-api-key\", \n",
    "            env_var=\"TOGETHER_API_KEY\",\n",
    "            mount_requirement=union.Secret.MountType.ENV_VAR,\n",
    "        ),\n",
    "        fl.Secret(\n",
    "            key=\"milvus-uri\",\n",
    "            env_var=\"MILVUS_URI\",\n",
    "            mount_requirement=union.Secret.MountType.ENV_VAR,\n",
    "        ),\n",
    "        fl.Secret(\n",
    "            key=\"milvus-token\",\n",
    "            env_var=\"MILVUS_TOKEN\",\n",
    "            mount_requirement=union.Secret.MountType.ENV_VAR,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "class Document(BaseModel):\n",
    "    idx: int\n",
    "    title: str\n",
    "    url: str\n",
    "    content: Optional[str] = None\n",
    "    chunks: Optional[list[str]] = None\n",
    "    prompts: Optional[list[str]] = None\n",
    "    contextual_chunks: Optional[list[str]] = None\n",
    "    tokens: Optional[list[list[int]]] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by defining an actor task to parse the main page of Paul Graham's essays. This task extracts a list of document titles and their respective URLs. Since actor tasks run within the shared actor environment we set up earlier, they efficiently reuse the same container and environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@actor.task\n",
    "def parse_main_page(\n",
    "    base_url: str, articles_url: str, local: bool = False\n",
    ") -> list[Document]:\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    assert base_url.endswith(\"/\"), f\"Base URL must end with a slash: {base_url}\"\n",
    "    response = requests.get(urljoin(base_url, articles_url))\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    td_cells = soup.select(\"table > tr > td > table > tr > td\")\n",
    "    documents = []\n",
    "\n",
    "    idx = 0\n",
    "    for td in td_cells:\n",
    "        img = td.find(\"img\")\n",
    "        if img and int(img.get(\"width\", 0)) <= 15 and int(img.get(\"height\", 0)) <= 15:\n",
    "            a_tag = td.find(\"font\").find(\"a\") if td.find(\"font\") else None\n",
    "            if a_tag:\n",
    "                documents.append(\n",
    "                    Document(\n",
    "                        idx=idx, title=a_tag.text, url=urljoin(base_url, a_tag[\"href\"])\n",
    "                    )\n",
    "                )\n",
    "                idx += 1\n",
    "\n",
    "    if local:\n",
    "        return documents[:3]\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define an actor task to scrape the content of each document. Using the list of URLs gathered in the previous step, this task extracts the full text of the essays, ensuring that all relevant content is retrieved for further processing.\n",
    "\n",
    "We also set `retries` to `3`, meaning the task will be retried three times before the error is propagated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@actor.task(retries=3)\n",
    "def scrape_pg_essays(document: Document) -> Document:\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    try:\n",
    "        response = requests.get(document.url)\n",
    "    except Exception as e:\n",
    "        raise FlyteRecoverableException(f\"Failed to scrape {document.url}: {str(e)}\")\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    content = soup.find(\"font\")\n",
    "\n",
    "    text = None\n",
    "    if content:\n",
    "        text = \" \".join(content.get_text().split())\n",
    "    document.content = text\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, define an actor task to create chunks for each document. Chunks are necessary because we need to append context to each chunk, ensuring the RAG app can process the information effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@actor.task(cache=True, cache_version=\"0.2\")\n",
    "def create_chunks(document: Document, chunk_size: int, overlap: int) -> Document:\n",
    "    if document.content:\n",
    "        content_chunks = [\n",
    "            document.content[i : i + chunk_size]\n",
    "            for i in range(0, len(document.content), chunk_size - overlap)\n",
    "        ]\n",
    "        document.chunks = content_chunks\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use Together.AI to generate context for each chunk of text, using the secret we initialized earlier. The system retrieves relevant context based on the entire document, ensuring accurate and meaningful outputs.\n",
    "\n",
    "Notice that we set [`cache`](https://docs.union.ai/serverless/user-guide/core-concepts/caching) to `True` for this task to avoid re-running the execution for the same inputs. This ensures that if the document and model remain unchanged, the outputs are retrieved directly from the cache, improving efficiency.\n",
    "\n",
    "Once the context is generated, we map the chunks back to their respective documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@actor.task(cache=True, cache_version=\"0.4\")\n",
    "def generate_context(document: Document, model: str) -> Document:\n",
    "    from together import Together\n",
    "\n",
    "    CONTEXTUAL_RAG_PROMPT = \"\"\"\n",
    "Given the document below, we want to explain what the chunk captures in the document.\n",
    "\n",
    "{WHOLE_DOCUMENT}\n",
    "\n",
    "Here is the chunk we want to explain:\n",
    "\n",
    "{CHUNK_CONTENT}\n",
    "\n",
    "Answer ONLY with a succinct explanation of the meaning of the chunk in the context of the whole document above.\n",
    "\"\"\"\n",
    "\n",
    "    client = Together(api_key=os.getenv(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "    contextual_chunks = [\n",
    "        f\"{response.choices[0].message.content} {chunk}\"\n",
    "        for chunk in (document.chunks or [])\n",
    "        for response in [\n",
    "            client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": CONTEXTUAL_RAG_PROMPT.format(\n",
    "                            WHOLE_DOCUMENT=document.content,\n",
    "                            CHUNK_CONTENT=chunk,\n",
    "                        ),\n",
    "                    }\n",
    "                ],\n",
    "                temperature=1,\n",
    "            )\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Assign the contextual chunks back to the document\n",
    "    document.contextual_chunks = contextual_chunks if contextual_chunks else None\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an embedding function to generate embeddings for each chunk. This function converts the chunks into vector representations, which we can store in a vector database for efficient retrieval and processing.\n",
    "\n",
    "Next, we create a vector index and store the embeddings in the [Milvus](https://milvus.io/) vector database. For each embedding, we store the ID, document, and document title. These details ensure the embeddings are ready for efficient retrieval during the RAG process.\n",
    "\n",
    "By setting `cache` to `True`, we avoid redundant upserts or inserts for the same document. Instead, we can add new records or update existing ones only if the content has changed. This approach keeps the vector database up-to-date efficiently, minimizing resource usage while maintaining accuracy.\n",
    "\n",
    "Note: We're using the Milvus hosted vector database to store the embeddings. However, you can replace it with any vector database of your choice based on your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "\n",
    "def get_embedding(chunk: str, model_api_string: str):\n",
    "    client = Together(\n",
    "        api_key=os.getenv(\"TOGETHER_API_KEY\")\n",
    "    )\n",
    "    outputs = client.embeddings.create(\n",
    "        input=chunk,\n",
    "        model=model_api_string,\n",
    "    )\n",
    "    return outputs.data[0].embedding\n",
    "\n",
    "\n",
    "@actor.task(cache=True, cache_version=\"0.19\", retries=5)\n",
    "def create_vector_index(\n",
    "    document: Document, model_api_string: str, local: bool = False\n",
    ") -> Document:\n",
    "    from pymilvus import DataType, MilvusClient\n",
    "\n",
    "    if local:\n",
    "        client = MilvusClient(\"test_milvus.db\")\n",
    "    else:\n",
    "        try:\n",
    "            client = MilvusClient(uri=os.getenv(\"MILVUS_URI\"), token=os.getenv(\"MILVUS_TOKEN\"))\n",
    "        except Exception as e:\n",
    "            raise FlyteRecoverableException(\n",
    "                f\"Failed to connect to Milvus: {e}\"\n",
    "            )\n",
    "\n",
    "    collection_name = \"paul_graham_collection\"\n",
    "\n",
    "    if not client.has_collection(collection_name):\n",
    "        schema = client.create_schema()\n",
    "        schema.add_field(\n",
    "            \"id\", DataType.INT64, is_primary=True, auto_id=True\n",
    "        )\n",
    "        schema.add_field(\"document_index\", DataType.VARCHAR, max_length=255)\n",
    "        schema.add_field(\"embedding\", DataType.FLOAT_VECTOR, dim=1024)\n",
    "        schema.add_field(\"title\", DataType.VARCHAR, max_length=255)\n",
    "        index_params = client.prepare_index_params()\n",
    "        index_params.add_index(\"embedding\", metric_type=\"COSINE\")\n",
    "\n",
    "        client.create_collection(collection_name, dimension=512, schema=schema, index_params=index_params)\n",
    "\n",
    "    if not document.contextual_chunks:\n",
    "        return document  # Exit early if there are no contextual chunks\n",
    "    \n",
    "    # Generate embeddings for chunks\n",
    "    embeddings = [get_embedding(chunk[:512], model_api_string) for chunk in document.contextual_chunks] # NOTE: Trimming the chunk for the embedding model's context window\n",
    "    embeddings_np = np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "    ids = [\n",
    "        f\"id{document.idx}_{chunk_idx}\"\n",
    "        for chunk_idx, _ in enumerate(document.contextual_chunks)\n",
    "    ]\n",
    "    titles = [document.title] * len(document.contextual_chunks)\n",
    "\n",
    "    client.upsert(\n",
    "        collection_name,\n",
    "        [\n",
    "            {\"id\": index, \"document_index\": document_index, \"embedding\": embedding, \"title\": title}\n",
    "            for index, (document_index, embedding, title) in enumerate(zip(ids, embeddings_np.tolist(), titles))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we create a BM25S keyword index to organize the document chunks. This index is great for keyword-based searches and works well alongside vector indexing. We also store a mapping between document IDs and their corresponding contextual chunk data, making it easier to retrieve content during the RAG process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@actor.task(cache=True, cache_version=\"0.5\")\n",
    "def create_bm25s_index(documents: list[Document]) -> tuple[FlyteDirectory, FlyteFile]:\n",
    "    import json\n",
    "    import bm25s\n",
    "\n",
    "    # Prepare data for JSON\n",
    "    data = {\n",
    "        f\"id{doc_idx}_{chunk_idx}\": contextual_chunk\n",
    "        for doc_idx, document in enumerate(documents)\n",
    "        if document.contextual_chunks\n",
    "        for chunk_idx, contextual_chunk in enumerate(document.contextual_chunks)\n",
    "    }\n",
    "\n",
    "    retriever = bm25s.BM25(corpus=list(data.values()))\n",
    "    retriever.index(bm25s.tokenize(list(data.values())))\n",
    "\n",
    "    ctx = union.current_context()\n",
    "    working_dir = Path(ctx.working_directory)\n",
    "    bm25s_index_dir = working_dir / \"bm25s_index\"\n",
    "    contextual_chunks_json = working_dir / \"contextual_chunks.json\"\n",
    "\n",
    "    retriever.save(str(bm25s_index_dir))\n",
    "\n",
    "    # Write the data to a JSON file\n",
    "    with open(contextual_chunks_json, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    return FlyteDirectory(path=bm25s_index_dir), FlyteFile(contextual_chunks_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a [standard workflow](https://docs.union.ai/serverless/user-guide/core-concepts/workflows/standard-workflows) to execute these tasks in sequence. By using [map tasks](https://docs.union.ai/serverless/user-guide/core-concepts/tasks/task-types#map-tasks), we run operations in parallel while respecting the resource constraints of each task. This approach **significantly improves execution speed**. We set the concurrency to 2, meaning two tasks will run in parallel. Note that the replica count for actors is set to 10, but this can be overridden at the map task level. We're doing this because having too many parallel clients could cause server availability issues.\n",
    "\n",
    "The final output of this workflow includes the BM25S keyword index and the contextual chunks mapping file, both returned as [Union Artifacts](https://docs.union.ai/serverless/user-guide/core-concepts/artifacts). The Artifact Service automatically indexes and assigns semantic meaning to all outputs from Union tasks and workflow executions, such as models, files, or other data. This makes it easy to track, access, and orchestrate pipelines directly through their outputs. In this case, the keyword index and file artifacts are directly used during app serving.\n",
    "\n",
    "We also set up a retrieval task to fetch embeddings for local execution. Once everything’s in place, we run the workflow and the retrieval task locally, producing a set of relevant chunks.\n",
    "\n",
    "One advantage of running locally is that all tasks and workflows are Python functions, making it easy to test everything before moving to production. This approach allows you to experiment locally and then deploy the same workflow in a production environment, ensuring it’s production-ready. You get the flexibility to test and refine your workflow without compromising on the capabilities needed for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samhitaalla/.pyenv/versions/contextual-rag-example/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetrievalResults(vector_results=[[\"{'id': 26, 'distance': 0.7850886583328247, 'entity': {'document_index': 'id2_26', 'title': 'When To Do What You Love'}}\", \"{'id': 19, 'distance': 0.6830964088439941, 'entity': {'document_index': 'id2_19', 'title': 'When To Do What You Love'}}\", \"{'id': 15, 'distance': 0.637735903263092, 'entity': {'document_index': 'id2_15', 'title': 'When To Do What You Love'}}\", \"{'id': 25, 'distance': 0.6253657341003418, 'entity': {'document_index': 'id2_25', 'title': 'When To Do What You Love'}}\", \"{'id': 18, 'distance': 0.625055730342865, 'entity': {'document_index': 'id2_18', 'title': 'When To Do What You Love'}}\"], [\"{'id': 30, 'distance': 0.5354647636413574, 'entity': {'document_index': 'id2_30', 'title': 'When To Do What You Love'}}\", \"{'id': 34, 'distance': 0.530750036239624, 'entity': {'document_index': 'id2_34', 'title': 'When To Do What You Love'}}\", \"{'id': 136, 'distance': 0.5282883644104004, 'entity': {'document_index': 'id0_136', 'title': 'The Origins of Wokeness'}}\", \"{'id': 5, 'distance': 0.5262698531150818, 'entity': {'document_index': 'id2_5', 'title': 'When To Do What You Love'}}\", \"{'id': 127, 'distance': 0.5256807208061218, 'entity': {'document_index': 'id0_127', 'title': 'The Origins of Wokeness'}}\"]], bm25s_results=[['When unsure, make uncertainty-proof choices by opting for options that expand future possibilities, essentially \"staying upwind.\" do in the face of uncertainty is to make choices that are uncertainty-proof. The less sure you are about what to do, the more important it is to choose options that give you more options in the future. I call this \"staying upwind.\" If you\\'re unsure w', \"This suggests that regardless of how helpful being told what to pursue ultimately is, it doesn't address the difficulty of figuring out what to do when all other factors are equal or when no clear distinction exists between what interests you and what's financially driven, emphasizing the need for more guidance on approaching this uncertainty.  this is good advice so far as it goes, but that's where it usually ends. No one tells you how to figure out what to work on, or how hard this can be.What do you do in the face of uncertainty? Get more certainty. And probably the best way to do that \", \"This chunk emphasizes the importance of choosing work that aligns with one's genuine interests to avoid wasted time and energy among colleagues, and instead experience inspiration from like-minded people.  from the outside. Whereas if you choose work you're genuinely interested in, you'll be surrounded mostly by other people who are genuinely interested in it, and that will make it extra inspiring. [3]The other thing you do in the face of uncertainty \", \"This chunk advises individuals to take the first step towards figuring out what to work on by starting to do it in some form, regardless of whether they have a job or internship lined up. It emphasizes that exploring one's interests early on is crucial and that procrastination can lead to being stuck in uncertainty for years, making it harder to find the right path. ng it in some form yourself. And since figuring out what to work on is a problem that could take years to solve, the sooner you start, the better.One useful trick for judging different kinds of work is to look at who your colleagues will be. You'll b\", \"The speaker argues that people often struggle between pursuing their passions versus a potentially lucrative career path due to ignorance, and that this ignorance manifests in three areas: a lack of understanding of one's own values and interests, limited knowledge of available career options, and uncertainty about one's own abilities and potential for success.  which path to take, it's almost always due to ignorance. In fact you're usually suffering from three kinds of ignorance simultaneously: you don't know what makes you happy, what the various kinds of work are really like, or how well you could do the\"], [\"The author predicts that in a couple decades, there won't be many people who can write, as technology (particularly AI) will render writing unnecessary, leading to a world divided between those who can write and those who can't. October 2024I'm usually reluctant to make predictions about technology, but I feel fairly confident about this one: in a couple decades there won't be many people who can write.One of the strangest things you learn if you're a writer is how many peop\", 'This chunk states that even though many people won\\'t be able to write, a new distinction will emerge between \"smart people\" who choose to write and those who don\\'t. l strong people, but only those who choose to be.It will be the same with writing. There will still be smart people, but only those who choose to be.Thanks to Jessica Livingston, Ben Miller, and Robert Morris for reading drafts of this.', 'The author believes that as AI replaces humans in writing tasks, the concept of \"good writing\" will no longer exist in the middle ground between skilled and unskilled writers, leaving society with two categories: those who can write creatively and produce good writing (writes), and those who cannot write at all (write-nots). tes and write-nots. There will still be some people who can write. Some of us like it. But the middle ground between those who are good at writing and those who can\\'t write at all will disappear. Instead of good writers, ok writers, and people who ca', \"The author is questioning whether a world that separates skilled writers (good writers) from those not skilled enough to write (people who can't write) is inherently bad. They point out that this is not unique in history, where skills became obsolete due to technology, like blacksmiths, but argue that the importance of writing as a thinking process makes this separation particularly concerning.  ok writers, and people who can't write, there will just be good writers and people who can't write.Is that so bad? Isn't it common for skills to disappear when technology makes them obsolete? There aren't many blacksmiths left, and it doesn't seem t\", \"The author argues that a world divided into people who can write (write-nots) and those who can't will pose a danger similar to the one posed by a world where physical strength favors those who dedicate time to training, emphasizing the value of writing as a mental activity and a producer of deep thinking.  better than Leslie Lamport did: If you're thinking without writing, you only think you're thinking. So a world divided into writes and write-nots is more dangerous than it sounds. It will be a world of thinks and think-nots. I know which half I want\"]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Ensure the secret (together API key) is present in the .env file\n",
    "\n",
    "BM25Index = Artifact(name=\"bm25s-index\")\n",
    "ContextualChunksJSON = Artifact(name=\"contextual-chunks-json\")\n",
    "\n",
    "\n",
    "@union.workflow\n",
    "def build_indices_wf(\n",
    "    base_url: str = \"https://paulgraham.com/\",\n",
    "    articles_url: str = \"articles.html\",\n",
    "    model_api_string: str = \"BAAI/bge-large-en-v1.5\",\n",
    "    chunk_size: int = 250,\n",
    "    overlap: int = 30,\n",
    "    model: str = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\",\n",
    "    local: bool = True,\n",
    ") -> tuple[\n",
    "    Annotated[FlyteDirectory, BM25Index], Annotated[FlyteFile, ContextualChunksJSON]\n",
    "]:\n",
    "    tocs = parse_main_page(base_url=base_url, articles_url=articles_url, local=local)\n",
    "    scraped_content = union.map_task(scrape_pg_essays, concurrency=2)(document=tocs)\n",
    "    chunks = union.map_task(\n",
    "        functools.partial(create_chunks, chunk_size=chunk_size, overlap=overlap)\n",
    "    )(document=scraped_content)\n",
    "    contextual_chunks = union.map_task(functools.partial(generate_context, model=model))(\n",
    "        document=chunks\n",
    "    )\n",
    "    union.map_task(\n",
    "        functools.partial(\n",
    "            create_vector_index, model_api_string=model_api_string, local=local\n",
    "        ), \n",
    "        concurrency=2\n",
    "    )(document=contextual_chunks)\n",
    "    bm25s_index, contextual_chunks_json_file = create_bm25s_index(\n",
    "        documents=contextual_chunks\n",
    "    )\n",
    "    return bm25s_index, contextual_chunks_json_file\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RetrievalResults:\n",
    "    vector_results: list[list[str]]\n",
    "    bm25s_results: list[list[str]]\n",
    "\n",
    "\n",
    "@union.task\n",
    "def retrieve(\n",
    "    bm25s_index: FlyteDirectory,\n",
    "    contextual_chunks_data: FlyteFile,\n",
    "    model_api_string: str = \"BAAI/bge-large-en-v1.5\",\n",
    "    queries: list[str] = [\n",
    "        \"What to do in the face of uncertainty?\",\n",
    "        \"Why won't people write?\",\n",
    "    ],\n",
    ") -> RetrievalResults:\n",
    "    import json\n",
    "\n",
    "    import bm25s\n",
    "    import numpy as np\n",
    "    from pymilvus import MilvusClient\n",
    "\n",
    "    client = MilvusClient(\"test_milvus.db\")    \n",
    "    \n",
    "    # Generate embeddings for the queries using Together\n",
    "    query_embeddings = [\n",
    "        get_embedding(query, model_api_string) for query in queries\n",
    "    ]\n",
    "    query_embeddings_np = np.array(query_embeddings, dtype=np.float32)\n",
    "\n",
    "    collection_name = \"paul_graham_collection\" \n",
    "    results = client.search(\n",
    "        collection_name, \n",
    "        query_embeddings_np, \n",
    "        limit=5, \n",
    "        search_params={\"metric_type\": \"COSINE\"}, \n",
    "        anns_field=\"embedding\",\n",
    "        output_fields=[\"document_index\", \"title\"]\n",
    "    )\n",
    "\n",
    "    # Load BM25S index\n",
    "    retriever = bm25s.BM25()\n",
    "    bm25_index = retriever.load(save_dir=bm25s_index.download())\n",
    "\n",
    "    # Load contextual chunk data\n",
    "    with open(contextual_chunks_data, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        contextual_chunks_data_dict = json.load(json_file)\n",
    "\n",
    "    # Perform BM25S-based retrieval\n",
    "    bm25s_idx_result = bm25_index.retrieve(\n",
    "        query_tokens=bm25s.tokenize(queries),\n",
    "        k=5,\n",
    "        corpus=np.array(list(contextual_chunks_data_dict.values())),\n",
    "    )\n",
    "\n",
    "    # Return results as a dataclass\n",
    "    return RetrievalResults(\n",
    "        vector_results=results,\n",
    "        bm25s_results=bm25s_idx_result.documents.tolist(),\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bm25s_index, contextual_chunks_data = build_indices_wf()\n",
    "    results = retrieve(\n",
    "        bm25s_index=bm25s_index, contextual_chunks_data=contextual_chunks_data\n",
    "    )\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote execution\n",
    "\n",
    "To provide the Together.AI API key to the actor during remote execution, we send it as a [secret](https://docs.union.ai/serverless/user-guide/development-cycle/managing-secrets#creating-secrets). We can create this secret using the Union CLI before running the workflow. Simply run the following commands:\n",
    "```\n",
    "union create secret together-api-key\n",
    "```\n",
    "\n",
    "To run the workflow remotely on a Union cluster, we start by logging into the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔐 \u001b[33mConfiguration saved to \u001b[0m\u001b[33m/Users/samhitaalla/.union/\u001b[0m\u001b[33mconfig.yaml\u001b[0m\n",
      "Login successful into \u001b[1;32mserverless\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!union create login --serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we initialize a Union remote object to execute the workflow on the cluster. The [UnionRemote](https://docs.union.ai/serverless/user-guide/development-cycle/union-remote) Python API supports functionality similar to that of the Union CLI, enabling you to manage Union workflows, tasks, launch plans and artifacts from within your Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">11:39:46.489864 </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> remote.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">291</span> - Jupyter notebook and interactive task  \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                </span>         support is still alpha.                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m11:39:46.489864\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m remote.py:\u001b[1;36m291\u001b[0m - Jupyter notebook and interactive task  \n",
       "\u001b[2;36m                \u001b[0m         support is still alpha.                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from union.remote import UnionRemote\n",
    "\n",
    "remote = UnionRemote(default_project=\"default\", default_domain=\"development\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_execution = remote.execute(build_indices_wf, inputs={\"local\": False})\n",
    "print(indices_execution.execution_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a launch plan to run the workflow daily. A [launch plan](https://docs.union.ai/serverless/user-guide/core-concepts/launch-plans/) serves as a template for invoking the workflow. \n",
    "\n",
    "The scheduled launch plan ensures that the vector database and keyword index are regularly updated, keeping the data fresh and synchronized.\n",
    "\n",
    "Be sure to note the `version` field when registering the launch plan. Each Union entity (task, workflow, launch plan) is automatically versioned, as every entity is associated with a version by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = fl.LaunchPlan.get_or_create(\n",
    "    workflow=build_indices_wf,\n",
    "    name=\"vector_db_ingestion_activate\",\n",
    "    schedule=fl.CronSchedule(\n",
    "        schedule=\"0 1 * * *\"\n",
    "    ),  # Run every day to update the databases\n",
    "    auto_activate=True,\n",
    ")\n",
    "\n",
    "registered_lp = remote.register_launch_plan(\n",
    "    entity=lp, version=\"v1\" # TODO: remove version\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy apps\n",
    "\n",
    "We deploy the FastAPI and Gradio applications to serve the RAG app with Union. Union allows us to build and serve our own web apps, though it's worth noting that serving on Union is still an experimental feature. FastAPI is used to define the endpoint for serving the app, while Gradio is used to create the user interface.\n",
    "\n",
    "When defining the app, we can specify inputs, images (using `ImageSpec`), resources to assign to the app, secrets, replicas, and more. We can organize the app specs into separate files. The FastAPI app spec is available in the `fastapi_app.py` file, and the Gradio app spec is in the `gradio_app.py` file.\n",
    "\n",
    "We retrieve the artifacts and send them as inputs to the FastAPI app. We can then retrieve the app's endpoint to use in the other app. Finally, we either create the app if it doesn't already exist or update it if it does.\n",
    "\n",
    "While we’re using FastAPI and Gradio here, you can use any Python-based front-end and API frameworks to define your apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from union.app import App, Input\n",
    "\n",
    "load_dotenv()  # Ensure you add REGISTRY to the .env file. TODO: remote image builder\n",
    "\n",
    "\n",
    "fastapi_app = App(\n",
    "    name=\"contextual-rag-fastapi\",\n",
    "    inputs=[\n",
    "        Input(\n",
    "            name=\"bm25s_index\",\n",
    "            value=BM25Index.query(),\n",
    "            download=True,\n",
    "            env_var=\"BM25S_INDEX\",\n",
    "        ),\n",
    "        Input(\n",
    "            name=\"contextual_chunks_json\",\n",
    "            value=ContextualChunksJSON.query(),\n",
    "            download=True,\n",
    "            env_var=\"CONTEXTUAL_CHUNKS_JSON\",\n",
    "        ),\n",
    "    ],\n",
    "    container_image=union.ImageSpec(\n",
    "        name=\"contextual-rag-fastapi\",\n",
    "        registry=os.getenv(\"REGISTRY\"),\n",
    "        packages=[\n",
    "            \"together\",\n",
    "            \"bm25s\",\n",
    "            \"pymilvus\",\n",
    "            \"uvicorn[standard]\",\n",
    "            \"fastapi[standard]\",\n",
    "            \"git+https://github.com/flyteorg/flytekit.git@4208a641debb0334c49c9331bcc4d98ed5c45d12\", # TODO: Update flytekit version\n",
    "            \"union-runtime>=0.1.10\",\n",
    "        ],\n",
    "        apt_packages=[\"git\"],\n",
    "    ),\n",
    "    limits=union.Resources(cpu=\"1\", mem=\"3Gi\"),\n",
    "    port=8080,\n",
    "    include=[\"fastapi_app.py\"],\n",
    "    args=[\"uvicorn\", \"fastapi_app:app\", \"--port\", \"8080\"],\n",
    "    min_replicas=1,\n",
    "    max_replicas=1,\n",
    "    secrets=[\n",
    "        fl.Secret(\n",
    "            key=\"together-api-key\", \n",
    "            env_var=\"TOGETHER_API_KEY\", \n",
    "            mount_requirement=union.Secret.MountType.ENV_VAR\n",
    "        ),\n",
    "        fl.Secret(\n",
    "            key=\"milvus-uri\",\n",
    "            env_var=\"MILVUS_URI\",\n",
    "            mount_requirement=union.Secret.MountType.ENV_VAR,\n",
    "        ),\n",
    "        fl.Secret(\n",
    "            key=\"milvus-token\",\n",
    "            env_var=\"MILVUS_TOKEN\",\n",
    "            mount_requirement=union.Secret.MountType.ENV_VAR,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "gradio_app = App(\n",
    "    name=\"contextual-rag-gradio\",\n",
    "    inputs=[\n",
    "        Input(\n",
    "            name=\"fastapi_endpoint\",\n",
    "            # value=fastapi_app.query_endpoint(public=False),\n",
    "            value=\"http://default-development-contextual-rag-fastapi.samhita-alla.svc.cluster.local\", # TODO: Fix query endpoint\n",
    "            env_var=\"FASTAPI_ENDPOINT\",\n",
    "        )\n",
    "    ],\n",
    "    container_image=union.ImageSpec(\n",
    "        name=\"contextual-rag-gradio\",\n",
    "        registry=os.getenv(\"REGISTRY\"),\n",
    "        packages=[\"gradio\", \"union-runtime>=0.1.5\"],\n",
    "    ),\n",
    "    limits=union.Resources(cpu=\"1\", mem=\"1Gi\"),\n",
    "    port=8080,\n",
    "    include=[\"gradio_app.py\"],\n",
    "    args=[\n",
    "        \"python\",\n",
    "        \"gradio_app.py\",\n",
    "    ],\n",
    "    min_replicas=1,\n",
    "    max_replicas=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">13:38:54.680630 </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> remote.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">291</span> - Jupyter notebook and interactive task  \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                </span>         support is still alpha.                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m13:38:54.680630\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m remote.py:\u001b[1;36m291\u001b[0m - Jupyter notebook and interactive task  \n",
       "\u001b[2;36m                \u001b[0m         support is still alpha.                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ Deploying Application: <a href=\"https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-fastapi\" target=\"_blank\">contextual-rag-fastapi</a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ Deploying Application: \u001b]8;id=74331;https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-fastapi\u001b\\contextual-rag-fastapi\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🔎 Console URL: \n",
       "<a href=\"https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-fastapi\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-fastapi</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🔎 Console URL: \n",
       "\u001b]8;id=608337;https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-fastapi\u001b\\\u001b[4;94mhttps://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-fastapi\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[Status]</span> <span style=\"font-style: italic\">Pending:</span> OutOfDate: The Configuration is still working to reflect the latest desired specification.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1mStatus\u001b[0m\u001b[1m]\u001b[0m \u001b[3mPending:\u001b[0m OutOfDate: The Configuration is still working to reflect the latest desired specification.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[Status]</span> <span style=\"font-style: italic\">Pending:</span> TrafficNotMigrated: Traffic is not yet migrated to the latest revision.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1mStatus\u001b[0m\u001b[1m]\u001b[0m \u001b[3mPending:\u001b[0m TrafficNotMigrated: Traffic is not yet migrated to the latest revision.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[Status]</span> <span style=\"font-style: italic\">Pending:</span> IngressNotConfigured: Ingress has not yet been reconciled.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1mStatus\u001b[0m\u001b[1m]\u001b[0m \u001b[3mPending:\u001b[0m IngressNotConfigured: Ingress has not yet been reconciled.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[Status]</span> <span style=\"font-style: italic\">Pending:</span> Uninitialized: Waiting for load balancer to be ready\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1mStatus\u001b[0m\u001b[1m]\u001b[0m \u001b[3mPending:\u001b[0m Uninitialized: Waiting for load balancer to be ready\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[Status]</span> <span style=\"font-style: italic\">Started:</span> Service is ready\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1mStatus\u001b[0m\u001b[1m]\u001b[0m \u001b[3mStarted:\u001b[0m Service is ready\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Deployed Endpoint: <a href=\"https://quiet-hill-86295.apps.serverless-1.us-east-2.s.union.ai\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://quiet-hill-86295.apps.serverless-1.us-east-2.s.union.ai</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Deployed Endpoint: \u001b]8;id=704177;https://quiet-hill-86295.apps.serverless-1.us-east-2.s.union.ai\u001b\\\u001b[4;94mhttps://quiet-hill-86295.apps.serverless-1.us-east-2.s.union.ai\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ Deploying Application: <a href=\"https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-gradio\" target=\"_blank\">contextual-rag-gradio</a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ Deploying Application: \u001b]8;id=954595;https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-gradio\u001b\\contextual-rag-gradio\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🔎 Console URL: \n",
       "<a href=\"https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-gradio\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-gradio</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🔎 Console URL: \n",
       "\u001b]8;id=814440;https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-gradio\u001b\\\u001b[4;94mhttps://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-gradio\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[Status]</span> <span style=\"font-style: italic\">Pending:</span> OutOfDate: The Configuration is still working to reflect the latest desired specification.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1mStatus\u001b[0m\u001b[1m]\u001b[0m \u001b[3mPending:\u001b[0m OutOfDate: The Configuration is still working to reflect the latest desired specification.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[Status]</span> <span style=\"font-style: italic\">Pending:</span> IngressNotConfigured: Ingress has not yet been reconciled.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1mStatus\u001b[0m\u001b[1m]\u001b[0m \u001b[3mPending:\u001b[0m IngressNotConfigured: Ingress has not yet been reconciled.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[Status]</span> <span style=\"font-style: italic\">Pending:</span> Uninitialized: Waiting for load balancer to be ready\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1mStatus\u001b[0m\u001b[1m]\u001b[0m \u001b[3mPending:\u001b[0m Uninitialized: Waiting for load balancer to be ready\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[Status]</span> <span style=\"font-style: italic\">Started:</span> Service is ready\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1mStatus\u001b[0m\u001b[1m]\u001b[0m \u001b[3mStarted:\u001b[0m Service is ready\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Deployed Endpoint: <a href=\"https://steep-feather-4e234.apps.serverless-1.us-east-2.s.union.ai\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://steep-feather-4e234.apps.serverless-1.us-east-2.s.union.ai</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Deployed Endpoint: \u001b]8;id=411877;https://steep-feather-4e234.apps.serverless-1.us-east-2.s.union.ai\u001b\\\u001b[4;94mhttps://steep-feather-4e234.apps.serverless-1.us-east-2.s.union.ai\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from union.remote._app_remote import AppRemote\n",
    "\n",
    "app_remote = AppRemote(project=\"default\", domain=\"development\")\n",
    "\n",
    "app_remote.create_or_update(fastapi_app)\n",
    "app_remote.create_or_update(gradio_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The apps will be deployed at the URLs provided in the output, which you can access. Below are some example queries to test the Gradio application:\n",
    "\n",
    "- What did Paul Graham do growing up?\n",
    "- What did the author do during their time in art school?\n",
    "- Can you give me a summary of the author's life?\n",
    "- What did the author do during their time at Yale?\n",
    "- What did the author do during their time at YC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to stop the apps, here’s how you can do it:\n",
    "# app_remote.stop(name=\"contextual-rag-fastapi-app\")\n",
    "# app_remote.stop(name=\"contextual-rag-gradio-app\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contextual-rag-example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
