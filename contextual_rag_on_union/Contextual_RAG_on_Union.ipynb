{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Contextual RAG Workflow with Together.AI on Union\n",
    "\n",
    "This notebook walks you through building a Contextual RAG (Retrieval-Augmented Generation) workflow on Union using Together's embedding, reranker, and chat models. It ties together web scraping, embedding generation, and serving into one cohesive application.\n",
    "\n",
    "[Union](https://www.union.ai/) is a production-grade AI orchestrator that unifies data, models, and compute workflows into a single, intuitive platform. It makes scalable, enterprise-ready AI accessible to all teams.\n",
    "\n",
    "In this notebook, we take the [existing Contextual RAG Together app](https://docs.together.ai/docs/how-to-implement-contextual-rag-from-anthropic) and make it \"production-grade\" with Union — ready for enterprise deployment.\n",
    "\n",
    "## Workflow overview\n",
    "\n",
    "The workflow follows these steps:\n",
    "\n",
    "1. Fetches all links to Paul Graham's essays.\n",
    "2. Scrapes web content to retrieve the full text of the essays.\n",
    "3. Splits the text into smaller chunks for processing.\n",
    "4. Appends context from the relevant essay to each chunk.\n",
    "5. Generates embeddings and stores them in a hosted vector database.\n",
    "6. Creates a keyword index for efficient retrieval.\n",
    "7. Serves a FastAPI app to expose the RAG functionality.\n",
    "8. Provides a Gradio app, using the FastAPI endpoint, for an easy-to-use RAG interface.\n",
    "\n",
    "Later in the notebook, we’ll dive into the Union features that simplify this entire process.\n",
    "\n",
    "## Execution approach\n",
    "\n",
    "This workflow is designed for local execution first, allowing you to test and validate it before deploying and scaling it on a Union cluster. This staged approach ensures smooth transitions from development to production.\n",
    "\n",
    "Before running the workflow, make sure to install `union`:\n",
    "\n",
    "```\n",
    "pip install union\n",
    "```\n",
    "\n",
    "### Local execution\n",
    "\n",
    "First, we import the required dependencies to ensure the workflow runs smoothly. Next, we define an actor environment, as the workflow relies on actor tasks throughout the process.\n",
    "\n",
    "[Union Actors](https://docs.union.ai/serverless/user-guide/core-concepts/actors) let us reuse a container and its environment across tasks, avoiding the overhead of starting a new container for each task. In this workflow, we define a single actor and reuse it consistently since the underlying components don’t require independent scaling or separate environments.\n",
    "\n",
    "Within the actor environment, we specify the `ImageSpec`, which defines the container image that tasks in the workflow will use. With Union, every task runs in its own dedicated container, requiring a container image. Instead of manually creating a Dockerfile, we define the image specification in Python. When run on Union Serverless, the container image is built remotely, simplifying the setup.\n",
    "\n",
    "We also configure the actor’s replica count to 50, meaning 50 workers are provisioned to handle tasks, allowing up to 50 tasks to run in parallel, provided sufficient resources. The TTL (time to live) is set to 120 seconds, ensuring the actor remains active for this period when no tasks are being processed.\n",
    "\n",
    "Finally, we create a Pydantic `BaseModel` named `Document` to capture metadata for each document used by the RAG app. This model ensures consistent data structuring and smooth integration throughout the workflow.\n",
    "\n",
    "NOTE: Add your Together.ai API key (`together-api-key`) and the name of your registry to the `.env` file before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Annotated, Optional\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import flytekit as fl\n",
    "import requests\n",
    "import union\n",
    "from flytekit.core.artifact import Artifact\n",
    "from flytekit.types.directory import FlyteDirectory\n",
    "from flytekit.types.file import FlyteFile\n",
    "from pydantic import BaseModel\n",
    "from union.actor import ActorEnvironment\n",
    "\n",
    "TOGETHER_API_KEY = \"together-api-key\"\n",
    "\n",
    "actor = ActorEnvironment(\n",
    "    name=\"contextual-rag\",\n",
    "    replica_count=50,\n",
    "    ttl_seconds=120,\n",
    "    container_image=union.ImageSpec(\n",
    "        name=\"contextual-rag\",\n",
    "        registry=\"ghcr.io/unionai-oss\",\n",
    "        packages=[\n",
    "            \"together==1.3.10\",\n",
    "            \"beautifulsoup4==4.12.3\",\n",
    "            \"bm25s==0.2.5\",\n",
    "            \"pydantic>2\",\n",
    "            \"chromadb==0.5.23\",\n",
    "            \"union>=0.1.117\",\n",
    "        ],\n",
    "    ),\n",
    "    secret_requests=[\n",
    "        union.Secret(\n",
    "            key=TOGETHER_API_KEY, \n",
    "            env_var=TOGETHER_API_KEY,\n",
    "            mount_requirement=union.Secret.MountType.ENV_VAR,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "class Document(BaseModel):\n",
    "    idx: int\n",
    "    title: str\n",
    "    url: str\n",
    "    content: Optional[str] = None\n",
    "    chunks: Optional[list[str]] = None\n",
    "    prompts: Optional[list[str]] = None\n",
    "    contextual_chunks: Optional[list[str]] = None\n",
    "    tokens: Optional[list[list[int]]] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by defining an actor task to parse the main page of Paul Graham's essays. This task extracts a list of document titles and their respective URLs. Since actor tasks run within the shared actor environment we set up earlier, they efficiently reuse the same container and environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@actor.task\n",
    "def parse_main_page(\n",
    "    base_url: str, articles_url: str, local: bool = False\n",
    ") -> list[Document]:\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    assert base_url.endswith(\"/\"), f\"Base URL must end with a slash: {base_url}\"\n",
    "    response = requests.get(urljoin(base_url, articles_url))\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    td_cells = soup.select(\"table > tr > td > table > tr > td\")\n",
    "    documents = []\n",
    "\n",
    "    idx = 0\n",
    "    for td in td_cells:\n",
    "        img = td.find(\"img\")\n",
    "        if img and int(img.get(\"width\", 0)) <= 15 and int(img.get(\"height\", 0)) <= 15:\n",
    "            a_tag = td.find(\"font\").find(\"a\") if td.find(\"font\") else None\n",
    "            if a_tag:\n",
    "                documents.append(\n",
    "                    Document(\n",
    "                        idx=idx, title=a_tag.text, url=urljoin(base_url, a_tag[\"href\"])\n",
    "                    )\n",
    "                )\n",
    "                idx += 1\n",
    "\n",
    "    if local:\n",
    "        return documents[:2]\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define an actor task to scrape the content of each document. Using the list of URLs gathered in the previous step, this task extracts the full text of the essays, ensuring that all relevant content is retrieved for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@actor.task\n",
    "def scrape_pg_essays(document: Document) -> Document:\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    response = requests.get(document.url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    content = soup.find(\"font\")\n",
    "\n",
    "    text = None\n",
    "    if content:\n",
    "        text = \" \".join(content.get_text().split())\n",
    "    document.content = text\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, define an actor task to create chunks for each document. Chunks are necessary because we need to append context to each chunk, ensuring the RAG app can process the information effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@actor.task(cache=True, cache_version=\"0.2\")\n",
    "def create_chunks(document: Document, chunk_size: int, overlap: int) -> Document:\n",
    "    if document.content:\n",
    "        content_chunks = [\n",
    "            document.content[i : i + chunk_size]\n",
    "            for i in range(0, len(document.content), chunk_size - overlap)\n",
    "        ]\n",
    "        document.chunks = content_chunks\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use Together.AI to generate context for each chunk of text, using the secret we initialized earlier. The system retrieves relevant context based on the entire document, ensuring accurate and meaningful outputs.\n",
    "\n",
    "Notice that we set [`cache`](https://docs.union.ai/serverless/user-guide/core-concepts/caching) to `True` for this task to avoid re-running the execution for the same inputs. This ensures that if the document and model remain unchanged, the outputs are retrieved directly from the cache, improving efficiency.\n",
    "\n",
    "Once the context is generated, we map the chunks back to their respective documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@actor.task(cache=True, cache_version=\"0.4\")\n",
    "def generate_context(document: Document, model: str) -> Document:\n",
    "    from together import Together\n",
    "\n",
    "    CONTEXTUAL_RAG_PROMPT = \"\"\"\n",
    "Given the document below, we want to explain what the chunk captures in the document.\n",
    "\n",
    "{WHOLE_DOCUMENT}\n",
    "\n",
    "Here is the chunk we want to explain:\n",
    "\n",
    "{CHUNK_CONTENT}\n",
    "\n",
    "Answer ONLY with a succinct explanation of the meaning of the chunk in the context of the whole document above.\n",
    "\"\"\"\n",
    "\n",
    "    client = Together(api_key=os.getenv(TOGETHER_API_KEY))\n",
    "\n",
    "    contextual_chunks = [\n",
    "        f\"{response.choices[0].message.content} {chunk}\"\n",
    "        for chunk in (document.chunks or [])\n",
    "        for response in [\n",
    "            client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": CONTEXTUAL_RAG_PROMPT.format(\n",
    "                            WHOLE_DOCUMENT=document.content,\n",
    "                            CHUNK_CONTENT=chunk,\n",
    "                        ),\n",
    "                    }\n",
    "                ],\n",
    "                temperature=1,\n",
    "            )\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Assign the contextual chunks back to the document\n",
    "    document.contextual_chunks = contextual_chunks if contextual_chunks else None\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a custom embedding class to generate embeddings for each chunk. This class helps us convert the chunks into vector representations, which we can store in a vector database for efficient retrieval and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from together import Together\n",
    "\n",
    "\n",
    "class TogetherEmbedding(EmbeddingFunction):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = model_name\n",
    "        self.client = Together(\n",
    "            api_key=os.getenv(TOGETHER_API_KEY)\n",
    "        )\n",
    "\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        outputs = self.client.embeddings.create(\n",
    "            input=input,\n",
    "            model=self.model,\n",
    "        )\n",
    "        return [x.embedding for x in outputs.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a vector index and store the embeddings in the [Chroma](https://trychroma.com) vector database. For each embedding, we store the ID, document, and associated metadata. These details ensure the embeddings are ready for efficient retrieval during the RAG process.\n",
    "\n",
    "Since we want to run this workflow locally, we initialize a `PersistentClient` to store the embeddings on the local system. For production, though, it’s a good idea to host the vector database in the cloud to ensure better scalability.\n",
    "\n",
    "By setting `cache` to `True`, we avoid redundant upserts or inserts for the same document. Instead, we can add new records or update existing ones only if the content has changed. This approach keeps the vector database up-to-date efficiently, minimizing resource usage while maintaining accuracy.\n",
    "\n",
    "Note: We're using the Chroma hosted vector database to store the embeddings. However, you can replace it with any vector database of your choice based on your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@actor.task(cache=True, cache_version=\"0.19\")\n",
    "def create_vector_index(\n",
    "    document: Document, model_api_string: str, local: bool = False\n",
    ") -> Document:\n",
    "    import chromadb\n",
    "\n",
    "    if not local:\n",
    "        client = chromadb.HttpClient(host=...) # TODO: Add hosted ChromaDB endpoint \n",
    "    else:\n",
    "        client = chromadb.PersistentClient()\n",
    "\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=\"paul-graham-collection\",\n",
    "        metadata={\"hnsw:space\": \"cosine\", \"hnsw:search_ef\": 50},\n",
    "        embedding_function=TogetherEmbedding(model_name=model_api_string),\n",
    "    )\n",
    "\n",
    "    if not document.contextual_chunks:\n",
    "        return document  # Exit early if there are no contextual chunks\n",
    "\n",
    "    ids = [\n",
    "        f\"id{document.idx}_{chunk_idx}\"\n",
    "        for chunk_idx, _ in enumerate(document.contextual_chunks)\n",
    "    ]\n",
    "    documents = [\n",
    "        chunk[:512]  # NOTE: Trimming the chunk for the embedding model's context window\n",
    "        for chunk in document.contextual_chunks\n",
    "    ]\n",
    "    metadatas = [{\"title\": document.title} for _ in document.contextual_chunks]\n",
    "\n",
    "    # Add to the collection\n",
    "    collection.upsert(ids=ids, documents=documents, metadatas=metadatas)\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we create a BM25S keyword index to organize the document chunks. This index is great for keyword-based searches and works well alongside vector indexing. We also store a mapping between document IDs and their corresponding contextual chunk data, making it easier to retrieve content during the RAG process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@actor.task(cache=True, cache_version=\"0.5\")\n",
    "def create_bm25s_index(documents: list[Document]) -> tuple[FlyteDirectory, FlyteFile]:\n",
    "    import json\n",
    "    import bm25s\n",
    "\n",
    "    # Prepare data for JSON\n",
    "    data = {\n",
    "        f\"id{doc_idx}_{chunk_idx}\": contextual_chunk\n",
    "        for doc_idx, document in enumerate(documents)\n",
    "        if document.contextual_chunks\n",
    "        for chunk_idx, contextual_chunk in enumerate(document.contextual_chunks)\n",
    "    }\n",
    "\n",
    "    retriever = bm25s.BM25(corpus=list(data.values()))\n",
    "    retriever.index(bm25s.tokenize(list(data.values())))\n",
    "\n",
    "    ctx = union.current_context()\n",
    "    working_dir = Path(ctx.working_directory)\n",
    "    bm25s_index_dir = working_dir / \"bm25s_index\"\n",
    "    contextual_chunks_json = working_dir / \"contextual_chunks.json\"\n",
    "\n",
    "    retriever.save(str(bm25s_index_dir))\n",
    "\n",
    "    # Write the data to a JSON file\n",
    "    with open(contextual_chunks_json, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    return FlyteDirectory(path=bm25s_index_dir), FlyteFile(contextual_chunks_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a [standard workflow](https://docs.union.ai/serverless/user-guide/core-concepts/workflows/standard-workflows) to execute these tasks in sequence. By using [map tasks](https://docs.union.ai/serverless/user-guide/core-concepts/tasks/task-types#map-tasks), we run operations in parallel while respecting the resource constraints of each task. This approach significantly improves execution speed. Additionally, by storing embeddings in the hosted vector database in parallel, we achieve a 22x speedup compared to storing the records one at a time!\n",
    "\n",
    "The final output of this workflow includes the BM25S keyword index and the contextual chunks mapping file, both returned as [Union Artifacts](https://docs.union.ai/serverless/user-guide/core-concepts/artifacts). The Artifact Service automatically indexes and assigns semantic meaning to all outputs from Union tasks and workflow executions, such as models, files, or other data. This makes it easy to track, access, and orchestrate pipelines directly through their outputs. In this case, the keyword index and file artifacts are directly used during app serving.\n",
    "\n",
    "We also set up a retrieval task to fetch embeddings for local execution. Once everything’s in place, we run the workflow and the retrieval task locally, producing a set of relevant chunks.\n",
    "\n",
    "One advantage of running locally is that all tasks and workflows are Python functions, making it easy to test everything before moving to production. This approach allows you to experiment locally and then deploy the same workflow in a production environment, ensuring it’s production-ready. You get the flexibility to test and refine your workflow without compromising on the capabilities needed for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Ensure the secret (together API key) is present in the .env file\n",
    "\n",
    "BM25Index = Artifact(name=\"bm25s-index\")\n",
    "ContextualChunksJSON = Artifact(name=\"contextual-chunks-json\")\n",
    "\n",
    "\n",
    "@union.workflow\n",
    "def build_indices_wf(\n",
    "    base_url: str = \"https://paulgraham.com/\",\n",
    "    articles_url: str = \"articles.html\",\n",
    "    model_api_string: str = \"BAAI/bge-large-en-v1.5\",\n",
    "    chunk_size: int = 250,\n",
    "    overlap: int = 30,\n",
    "    model: str = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\",\n",
    "    local: bool = True,\n",
    ") -> tuple[\n",
    "    Annotated[FlyteDirectory, BM25Index], Annotated[FlyteFile, ContextualChunksJSON]\n",
    "]:\n",
    "    tocs = parse_main_page(base_url=base_url, articles_url=articles_url, local=local)\n",
    "    scraped_content = union.map_task(scrape_pg_essays)(document=tocs)\n",
    "    chunks = union.map_task(\n",
    "        functools.partial(create_chunks, chunk_size=chunk_size, overlap=overlap)\n",
    "    )(document=scraped_content)\n",
    "    contextual_chunks = union.map_task(functools.partial(generate_context, model=model))(\n",
    "        document=chunks\n",
    "    )\n",
    "    union.map_task(\n",
    "        functools.partial(\n",
    "            create_vector_index, model_api_string=model_api_string, local=local\n",
    "        )\n",
    "    )(document=contextual_chunks)\n",
    "    bm25s_index, contextual_chunks_json_file = create_bm25s_index(\n",
    "        documents=contextual_chunks\n",
    "    )\n",
    "    return bm25s_index, contextual_chunks_json_file\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RetrievalResults:\n",
    "    vector_results: list[list[str]]\n",
    "    bm25s_results: list[list[str]]\n",
    "\n",
    "\n",
    "@union.task\n",
    "def retrieve(\n",
    "    bm25s_index: FlyteDirectory,\n",
    "    contextual_chunks_data: FlyteFile,\n",
    "    model_api_string: str = \"BAAI/bge-large-en-v1.5\",\n",
    "    queries: list[str] = [\n",
    "        \"What to do in the face of uncertainty?\",\n",
    "        \"Why won't people write?\",\n",
    "    ],\n",
    ") -> RetrievalResults:\n",
    "    import json\n",
    "\n",
    "    import bm25s\n",
    "    import chromadb\n",
    "    import numpy as np\n",
    "\n",
    "    # Initialize ChromaDB client\n",
    "    client = chromadb.PersistentClient()\n",
    "\n",
    "    # Get the collection and set up the embedding function\n",
    "    collection_name = client.list_collections()[0].name\n",
    "    collection = client.get_collection(\n",
    "        collection_name,\n",
    "        embedding_function=TogetherEmbedding(model_name=model_api_string),\n",
    "    )\n",
    "\n",
    "    # Perform vector-based retrieval\n",
    "    vector_idx_result = collection.query(\n",
    "        query_texts=queries,\n",
    "        n_results=5,\n",
    "    )\n",
    "\n",
    "    # Load BM25S index\n",
    "    retriever = bm25s.BM25()\n",
    "    bm25_index = retriever.load(save_dir=bm25s_index.download())\n",
    "\n",
    "    # Load contextual chunk data\n",
    "    with open(contextual_chunks_data, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        contextual_chunks_data_dict = json.load(json_file)\n",
    "\n",
    "    # Perform BM25S-based retrieval\n",
    "    bm25s_idx_result = bm25_index.retrieve(\n",
    "        query_tokens=bm25s.tokenize(queries),\n",
    "        k=5,\n",
    "        corpus=np.array(list(contextual_chunks_data_dict.values())),\n",
    "    )\n",
    "\n",
    "    # Return results as a dataclass\n",
    "    return RetrievalResults(\n",
    "        vector_results=vector_idx_result[\"documents\"],\n",
    "        bm25s_results=bm25s_idx_result.documents.tolist(),\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bm25s_index, contextual_chunks_data = build_indices_wf()\n",
    "    results = retrieve(\n",
    "        bm25s_index=bm25s_index, contextual_chunks_data=contextual_chunks_data\n",
    "    )\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote execution\n",
    "\n",
    "To provide the Together.AI API key to the actor during remote execution, we send it as a [secret](https://docs.union.ai/serverless/user-guide/development-cycle/managing-secrets#creating-secrets). We can create this secret using the Union CLI before running the workflow. Simply run the following commands:\n",
    "```\n",
    "union create secret together-api-key\n",
    "```\n",
    "\n",
    "To run the workflow remotely on a Union cluster, we start by logging into the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union create login --serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we initialize a Union remote object to execute the workflow on the cluster. The [UnionRemote](https://docs.union.ai/serverless/user-guide/development-cycle/union-remote) Python API supports functionality similar to that of the Union CLI, enabling you to manage Union workflows, tasks, launch plans and artifacts from within your Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from union.remote import UnionRemote\n",
    "\n",
    "remote = UnionRemote(default_project=\"default\", default_domain=\"development\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_execution = remote.execute(build_indices_wf, inputs={\"local\": False})\n",
    "print(indices_execution.execution_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a launch plan to run the workflow daily. A [launch plan](https://docs.union.ai/serverless/user-guide/core-concepts/launch-plans/) serves as a template for invoking the workflow. \n",
    "\n",
    "The scheduled launch plan ensures that the vector database and keyword index are regularly updated, keeping the data fresh and synchronized.\n",
    "\n",
    "Be sure to note the `version` field when registering the launch plan. Each Union entity (task, workflow, launch plan) is automatically versioned, as every entity is associated with a version by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = union.LaunchPlan.get_or_create(\n",
    "    build_indices_wf,\n",
    "    name=\"vector_db_ingestion\",\n",
    "    schedule=fl.CronSchedule(\n",
    "        schedule=\"0 1 * * *\"\n",
    "    ),  # Run every day to update the databases\n",
    ")\n",
    "\n",
    "registered_lp = remote.register_launch_plan(\n",
    "    entity=lp, version=\"v1\"\n",
    ") \n",
    "remote.activate_launchplan(registered_lp.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy apps\n",
    "\n",
    "We deploy the FastAPI and Gradio applications to serve the RAG app with Union. Union allows us to build and serve our own web apps, though it's worth noting that serving on Union is still an experimental feature. FastAPI is used to define the endpoint for serving the app, while Gradio is used to create the user interface.\n",
    "\n",
    "When defining the app, we can specify inputs, images (using `ImageSpec`), resources to assign to the app, secrets, replicas, and more. We can organize the app specs into separate files. The FastAPI app spec is available in the `fastapi_app.py` file, and the Gradio app spec is in the `gradio_app.py` file.\n",
    "\n",
    "We retrieve the artifacts and send them as inputs to the FastAPI app. We can then retrieve the app's endpoint to use in the other app. Finally, we either create the app if it doesn't already exist or update it if it does.\n",
    "\n",
    "While we’re using FastAPI and Gradio here, you can use any Python-based front-end and API frameworks to define your apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from union.app import App, Input\n",
    "from union.remote._app_remote import AppRemote\n",
    "\n",
    "load_dotenv()  # Ensure you add REGISTRY to the .env file.\n",
    "\n",
    "app_remote = AppRemote(project=\"demo\", domain=\"development\")\n",
    "\n",
    "fastapi_app = App(\n",
    "    name=\"contextual-rag-fastapi-app\",\n",
    "    inputs=[\n",
    "        Input(\n",
    "            name=\"bm25s_index\",\n",
    "            value=BM25Index.query(),\n",
    "            auto_download=True,\n",
    "            env_var=\"BM25S_INDEX\",\n",
    "        ),\n",
    "        Input(\n",
    "            name=\"contextual_chunks_json\",\n",
    "            value=ContextualChunksJSON.query(),\n",
    "            auto_download=True,\n",
    "            env_var=\"CONTEXTUAL_CHUNKS_JSON\",\n",
    "        ),\n",
    "        Input(\n",
    "            name=\"chroma_db_endpoint\",\n",
    "            value=..., # TODO: Add hosted ChromaDB endpoint \n",
    "            env_var=\"CHROMA_DB_ENDPOINT\",\n",
    "        ),\n",
    "    ],\n",
    "    container_image=union.ImageSpec(\n",
    "        name=\"contextual-rag-fastapi\",\n",
    "        registry=os.getenv(\"REGISTRY\"),\n",
    "        packages=[\n",
    "            \"together\",\n",
    "            \"bm25s\",\n",
    "            \"chromadb\",\n",
    "            \"fastapi[standard]\",\n",
    "            \"union-runtime>=0.1.5\",\n",
    "        ],\n",
    "    ),\n",
    "    limits=union.Resources(cpu=\"3\", mem=\"10Gi\"),\n",
    "    port=8080,\n",
    "    include=[\"./fastapi_app.py\"],\n",
    "    command=[\"fastapi\", \"dev\", \"--port\", \"8080\"],\n",
    "    min_replicas=1,\n",
    "    max_replicas=1,\n",
    ")\n",
    "\n",
    "\n",
    "gradio_app = App(\n",
    "    name=\"contextual-rag-gradio-app\",\n",
    "    inputs=[\n",
    "        Input(\n",
    "            name=\"fastapi_endpoint\",\n",
    "            value=fastapi_app.query_endpoint(public=False),\n",
    "            env_var=\"FASTAPI_ENDPOINT\",\n",
    "        )\n",
    "    ],\n",
    "    container_image=union.ImageSpec(\n",
    "        name=\"contextual-rag-gradio\",\n",
    "        registry=os.getenv(\"REGISTRY\"),\n",
    "        packages=[\"gradio\", \"union-runtime>=0.1.5\"],\n",
    "    ),\n",
    "    limits=union.Resources(cpu=\"1\", mem=\"1Gi\"),\n",
    "    port=8080,\n",
    "    include=[\"./gradio_app.py\"],\n",
    "    command=[\n",
    "        \"python\",\n",
    "        \"gradio_app.py\",\n",
    "    ],\n",
    "    min_replicas=1,\n",
    "    max_replicas=1,\n",
    ")\n",
    "\n",
    "app_remote.create_or_update(fastapi_app)\n",
    "app_remote.create_or_update(gradio_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The apps are deployed at the provided URLs, which you can access. Below are some example queries to test the Gradio application:\n",
    "\n",
    "- What did Paul Graham do growing up?\n",
    "- What did the author do during their time in art school?\n",
    "- Can you give me a summary of the author's life?\n",
    "- What did the author do during their time at Yale?\n",
    "- What did the author do during their time at YC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to stop the apps, here’s how you can do it:\n",
    "# app_remote.stop(name=\"contextual-rag-fastapi-app\")\n",
    "# app_remote.stop(name=\"contextual-rag-gradio-app\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contextual-rag-example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
