{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Workflows and Recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os, json\n",
    "import together\n",
    "from pydantic import ValidationError\n",
    "from together import AsyncTogether, Together\n",
    "\n",
    "client = Together(api_key= \"abc\")\n",
    "async_client = AsyncTogether(api_key= \"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm(user_prompt : str, model : str, system_prompt : str = None):\n",
    "\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=4000,        \n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The capital of the United States of America (USA) is Washington, D.C. (short for District of Columbia).',\n",
       " 'The capital of the United States of America (USA) is Washington, D.C. (short for District of Columbia).']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The function below will call the reference LLMs in parallel\n",
    "async def run_llm_parallel(user_prompt : str, model : str, system_prompt : str = None):\n",
    "    \"\"\"Run a single LLM call with a reference model.\"\"\"\n",
    "    for sleep_time in [1, 2, 4]:\n",
    "        try:\n",
    "            messages = []\n",
    "            if system_prompt:\n",
    "                messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "            messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0.7,\n",
    "                max_tokens=2000,\n",
    "            )\n",
    "            break\n",
    "        except together.error.RateLimitError as e:\n",
    "            print(e)\n",
    "            await asyncio.sleep(sleep_time)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSON_llm(user_prompt : str, schema, system_prompt : str = None):\n",
    "    \n",
    "    try:\n",
    "        messages = []\n",
    "        if system_prompt:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "        messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "        \n",
    "        extract = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "            response_format={\n",
    "                \"type\": \"json_object\",\n",
    "                \"schema\": schema.model_json_schema(),\n",
    "            },\n",
    "        )\n",
    "        return json.loads(extract.choices[0].message.content)\n",
    "\n",
    "    except ValidationError as e:\n",
    "        error_message = f\"Failed to parse JSON: {e}\"\n",
    "        print(error_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Chaining Recipe\n",
    "A simple snippet of serial prompt chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "Relevant numerical information: \n",
      "- $12 (hourly wage)\n",
      "- 50 minutes (time worked, can be converted to hours for calculation: 50 / 60 = 5/6 hour)\n",
      "\n",
      "Step 2\n",
      "1. Convert 50 minutes to hours: 50 / 60 = 5/6 hour.\n",
      "2. Multiply the hourly wage by the time worked in hours: $12 * (5/6).\n",
      "3. Calculate the result of the multiplication to find the earnings.\n",
      "\n",
      "Step 3\n",
      "To find the earnings, we need to perform the multiplication of $12 and 5/6.\n",
      "\n",
      "First, convert the fraction to a decimal: 5/6 ≈ 0.83\n",
      "\n",
      "Then, multiply $12 by 0.83: \n",
      "$12 * 0.83 ≈ $9.96\n",
      "\n",
      "So, the earnings are approximately $9.96.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def serial_chain_workflow(input_query: str, prompt_chain : List[str]) -> List[str]:\n",
    "    \"\"\"Run a serial chain of LLM calls to address the `input_query` \n",
    "    using a prompts specified in a list `prompt_chain`.\n",
    "    \"\"\"\n",
    "    response_chain = []\n",
    "    response = input_query\n",
    "    for i, prompt in enumerate(prompt_chain):\n",
    "        print(f\"Step {i+1}\")\n",
    "        response = run_llm(f\"{prompt}\\nInput:\\n{response}\", model='meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo')\n",
    "        response_chain.append(response)\n",
    "        print(f\"{response}\\n\")\n",
    "    return response_chain\n",
    "\n",
    "# Toy Example\n",
    "\n",
    "question = \"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\"\n",
    "\n",
    "prompt_chain = [\"\"\"Given the math problem, ONLY extract any relevant numerical information and how it can be used.\"\"\",\n",
    "                \"\"\"Given the numberical information extracted, ONLY express the steps you would take to solve the problem.\"\"\",\n",
    "                \"\"\"Given the steps, express the final answer to the problem.\"\"\"]\n",
    "\n",
    "responses = serial_chain_workflow(question, prompt_chain)\n",
    "\n",
    "final_answer = responses[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing\n",
    "A simple snippet of the consitional routing workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router_workflow(input_query: str, routes : Dict[str, str]) -> str:\n",
    "    \"\"\" Given a `input_qeury` and a dictionary of `routes` containing options and details for each.\n",
    "    Selects the best model for the task and return the response from the model.\n",
    "    \"\"\"\n",
    "    ROUTER_PROMPT = \"\"\"Given a user prompt/query: {user_query}, select the best option out of the following routes:\n",
    "    {routes}. Answer only in JSON format.\"\"\"\n",
    "\n",
    "    # Create a schema from the routes dictionary\n",
    "    class Schema(BaseModel):\n",
    "        route: Literal[tuple(routes.keys())]\n",
    "    \n",
    "        reason: str = Field(\n",
    "            description=\"Short one-liner explanation why this route was selected for the task in the prompt/query.\"\n",
    "        )\n",
    "\n",
    "    # Call LLM to select route\n",
    "    selected_route = JSON_llm(ROUTER_PROMPT.format(user_query=input_query, routes=routes), Schema)\n",
    "    print(f\"Selcted route:{selected_route['route']}\\nReason: {selected_route['reason']}\\n\")\n",
    "\n",
    "    # Use LLM on selected route. \n",
    "    # Could also have different prompts that need to be used for each route.\n",
    "    response = run_llm(user_prompt= input_query, model = selected_route['route'])\n",
    "    print(f\"Response: {response}\\n\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1: Produce python snippet to check to see if a number is prime or not.\n",
      "\n",
      "========================================\n",
      "Seelction route:Qwen/Qwen2.5-Coder-32B-Instruct\n",
      " Reason: The task requires generating a Python code snippet to check if a number is prime or not, which falls under code generation tasks.\n",
      "\n",
      "Response: Certainly! Below is a Python function that checks whether a given number is prime or not:\n",
      "\n",
      "```python\n",
      "def is_prime(n):\n",
      "    \"\"\"Check if a number is prime.\"\"\"\n",
      "    if n <= 1:\n",
      "        return False\n",
      "    if n <= 3:\n",
      "        return True\n",
      "    if n % 2 == 0 or n % 3 == 0:\n",
      "        return False\n",
      "    i = 5\n",
      "    while i * i <= n:\n",
      "        if n % i == 0 or n % (i + 2) == 0:\n",
      "            return False\n",
      "        i += 6\n",
      "    return True\n",
      "\n",
      "# Example usage:\n",
      "number = 29\n",
      "if is_prime(number):\n",
      "    print(f\"{number} is a prime number.\")\n",
      "else:\n",
      "    print(f\"{number} is not a prime number.\")\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **Initial Checks**: \n",
      "   - Numbers less than or equal to 1 are not prime.\n",
      "   - Numbers 2 and 3 are prime.\n",
      "   \n",
      "2. **Divisibility Check**:\n",
      "   - If the number is divisible by 2 or 3, it is not prime.\n",
      "   \n",
      "3. **Loop through potential factors**:\n",
      "   - Starting from 5, check divisibility up to the square root of `n`.\n",
      "   - The loop increments by 6 each time, checking `i` and `i + 2` (i.e., 5 and 7, 11 and 13, etc.), because all primes greater than 3 can be written in the form of 6k ± 1.\n",
      "\n",
      "This method is efficient for checking primality for reasonably large numbers.\n",
      "\n",
      "Task 2: Plan and provide a short itenary for a 2 week vacation in Europe.\n",
      "\n",
      "========================================\n",
      "Seelction route:Qwen/QwQ-32B-Preview\n",
      " Reason: Planning and providing a short itinerary for a 2-week vacation in Europe requires reasoning, planning, and multi-step tasks.\n",
      "\n",
      "Response: Planning a 2-week vacation in Europe can be both exciting and overwhelming due to the vast number of options available. Europe is rich in history, culture, and natural beauty, offering something for every type of traveler. To make the most of your time, it's essential to prioritize the destinations that interest you the most and plan a balanced itinerary that allows for both exploration and relaxation.\n",
      "\n",
      "### Step-by-Step Planning Guide\n",
      "\n",
      "1. **Define Your Interests:**\n",
      "   - History and Culture: Museums, castles, historical sites.\n",
      "   - Nature and Outdoors: Mountains, beaches, national parks.\n",
      "   - Food and Wine: Culinary experiences, wine tastings.\n",
      "   - City Life: Shopping, nightlife, contemporary culture.\n",
      "\n",
      "2. **Choose Countries and Cities:**\n",
      "   - Consider visa requirements, language barriers, and travel time between destinations.\n",
      "   - Popular choices include France, Italy, Spain, Germany, the UK, and the Netherlands.\n",
      "\n",
      "3. **Plan Your Route:**\n",
      "   - Start from the main entry point (e.g., London, Paris, Amsterdam).\n",
      "   - Plan a route that minimizes backtracking and efficient travel between cities.\n",
      "   - Consider internal flights or train routes for faster travel.\n",
      "\n",
      "4. **Allocate Time per Destination:**\n",
      "   - Spend at least 2-3 days in each city to explore adequately.\n",
      "   - Allow for travel days between destinations.\n",
      "\n",
      "5. **Book Accommodations:**\n",
      "   - Choose accommodations based on location, budget, and preferences.\n",
      "   - Book in advance, especially during peak seasons.\n",
      "\n",
      "6. **Research Attractions and Activities:**\n",
      "   - Make a list of must-see sights and experiences in each destination.\n",
      "   - Consider purchasing tickets in advance for popular attractions.\n",
      "\n",
      "7. **Budgeting:**\n",
      "   - Estimate costs for flights, accommodations, transportation, food, and activities.\n",
      "   - Set a daily budget to manage expenses.\n",
      "\n",
      "8. **Pack Smart:**\n",
      "   - Check the weather forecast for your destinations.\n",
      "   - Pack versatile clothing and comfortable shoes.\n",
      "   - Bring essential travel documents and copies.\n",
      "\n",
      "### Sample Itinerary\n",
      "\n",
      "#### Week 1: France and Italy\n",
      "\n",
      "**Day 1-3: Paris, France**\n",
      "- Arrival in Paris.\n",
      "- Visit the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral.\n",
      "- Stroll along the Champs-Élysées and enjoy French cuisine.\n",
      "\n",
      "**Day 4-6: Florence, Italy**\n",
      "- Fly from Paris to Florence.\n",
      "- Explore the Uffizi Gallery, Ponte Vecchio, and the Duomo.\n",
      "- Take a day trip to Tuscany for vineyard tours.\n",
      "\n",
      "**Day 7-9: Rome, Italy**\n",
      "- Travel from Florence to Rome by train.\n",
      "- Visit the Colosseum, Roman Forum, and Vatican City.\n",
      "- Enjoy pizza and gelato in Trastevere.\n",
      "\n",
      "#### Week 2: Spain and Portugal\n",
      "\n",
      "**Day 10-12: Barcelona, Spain**\n",
      "- Fly from Rome to Barcelona.\n",
      "- See Gaudí’s Sagrada Família, Park Güell, and Casa Batlló.\n",
      "- Relax on the beaches of Barceloneta.\n",
      "\n",
      "**Day 13-15: Lisbon, Portugal**\n",
      "- Travel from Barcelona to Lisbon by train or flight.\n",
      "- Explore Belem Tower, Jerónimos Monastery, and Alfama district.\n",
      "- Take a tram to Cristo Rei and enjoy Fado music.\n",
      "\n",
      "**Day 16: Departure**\n",
      "- Transfer to the airport for your return flight.\n",
      "\n",
      "### Tips for a Smooth Trip\n",
      "\n",
      "- **Travel Light:** Carry a backpack or rolling suitcase for ease of movement.\n",
      "- **Stay Connected:** Consider getting a local SIM card or international data plan.\n",
      "- **Learn Basic Phrases:** Knowing a few words in the local language can be helpful.\n",
      "- **Stay Flexible:** Be prepared for changes in plans due to weather or other factors.\n",
      "- **Enjoy the Journey:** Take time to soak in the culture and meet new people.\n",
      "\n",
      "Europe is a continent filled with diverse experiences, and a well-planned itinerary can ensure that you make the most of your two-week adventure. Whether you're exploring historic cities, indulging in local cuisine, or relaxing in natural美景, Europe has something to offer every traveler.\n",
      "\n",
      "Task 3: Write a short story about a dragon and a knight.\n",
      "\n",
      "========================================\n",
      "Seelction route:Gryphe/MythoMax-L2-13b\n",
      " Reason: The task requires story-telling and fantasy, which is best suited for the Gryphe/MythoMax-L2-13b model.\n",
      "\n",
      "Response: \n",
      " In a faraway kingdom, there once lived a mighty dragon named Drako. He was known for his fiery breath and sharp claws, terrorizing the nearby villages and towns. The king of the land decided to send his bravest knight, Sir William, to slay the beast and bring peace to his people.\n",
      "Sir William set out on his quest, armed with his sword and shield. After days of riding through the dense forest, he finally saw the dragon's lair. The knight dismounted from his horse and slowly approached the cave entrance. He took a deep breath, bracing himself for the battle ahead.\n",
      "As he stepped into the dark cave, he heard a loud roar that sent shivers down his spine. The dragon emerged from the shadows, its fiery eyes fixed on the knight. The beast let out another deafening roar, and Sir William readied his sword.\n",
      "What happened next surprised everyone. Instead of attacking, Drako spoke to the knight. He told him about how lonely he had been, how he had been driven to terrorize the kingdom out of desperation. Sir William listened intently, realizing that the dragon was not inherently evil but rather misunderstood.\n",
      "The knight proposed a truce between them, offering to help Drako find a new home and food if he promised to stop terrorizing the towns. Drako agreed, and together they left the cave.\n",
      "The people of the kingdom were amazed to see the dragon and the knight walking side by side. Sir William explained what had happened, and the king agreed to the truce. Drako was given a new home in a remote part of the kingdom, far away from the towns, and he was provided with food to sustain him.\n",
      "From that day forward, the dragon and the knight became unlikely friends. They would often visit each other, and the people of the kingdom knew that they were safe from harm. The legend of the dragon and the knight lived on, teaching everyone that even the fiercest of enemies could become friends if given the chance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_list = [\"Produce python snippet to check to see if a number is prime or not.\",\n",
    "               \"Plan and provide a short itenary for a 2 week vacation in Europe.\",\n",
    "               \"Write a short story about a dragon and a knight.\"]\n",
    "\n",
    "model_routes = {\n",
    "    \"Qwen/Qwen2.5-Coder-32B-Instruct\" : \"Best model choice for code generation tasks.\",\n",
    "    \"Gryphe/MythoMax-L2-13b\" : \"Best model choice for story-telling, role-playing and fantasy tasks.\",\n",
    "    \"Qwen/QwQ-32B-Preview\" : \"Best model for reasoning, planning and muilti-step tasks\",\n",
    "}\n",
    "\n",
    "for i, prompt in enumerate(prompt_list):\n",
    "    print(f\"Task {i+1}: {prompt}\\n\")\n",
    "    print(20*'==')\n",
    "    router_workflow(prompt, model_routes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Recipe\n",
    "A simple snippet of parallel agent workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def parallel_workflow(prompt : str, proposer_models : List[str], aggregator_model : str, aggregator_prompt: str):\n",
    "    \"\"\"Run a parallel chain of LLM calls to address the `input_query` \n",
    "    using a list of models specified in `models`.\n",
    "\n",
    "    Returns output from final aggregator model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Gather intermediate responses from proposer models\n",
    "    proposed_responses = await asyncio.gather(*[run_llm_parallel(prompt, model) for model in proposer_models])\n",
    "    \n",
    "    # Aggregate responses using an aggregator model\n",
    "    final_output = run_llm(user_prompt=prompt,\n",
    "                           model=aggregator_model,\n",
    "                           system_prompt=aggregator_prompt + \"\\n\" + \"\\n\".join(f\"{i+1}. {str(element)}\" for i, element in enumerate(proposed_responses)\n",
    "           ))\n",
    "    \n",
    "    return final_output, proposed_responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_models = [\n",
    "    \"microsoft/WizardLM-2-8x22B\",\n",
    "    \"Qwen/Qwen2.5-72B-Instruct-Turbo\",\n",
    "    \"google/gemma-2-27b-it\",\n",
    "    \"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "]\n",
    "\n",
    "user_prompt = \"\"\"Jenna and her mother picked some apples from their apple farm. \n",
    "Jenna picked half as many apples as her mom. If her mom got 20 apples, how many apples did they both pick?\"\"\"\n",
    "\n",
    "aggregator_model = \"deepseek-ai/DeepSeek-V3\"\n",
    "\n",
    "aggregator_system_prompt = \"\"\"You have been provided with a set of responses from various open-source models to the latest user query.\n",
    "Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information\n",
    "provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the\n",
    "given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured,\n",
    "coherent, and adheres to the highest standards of accuracy and reliability.\n",
    "\n",
    "Responses from models:\"\"\"\n",
    "\n",
    "answer, intermediate_reponses = await parallel_workflow(prompt = user_prompt, \n",
    "                                                        proposer_models = reference_models, \n",
    "                                                        aggregator_model = aggregator_model, \n",
    "                                                        aggregator_prompt = aggregator_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermetidate Response 1:\n",
      "\n",
      " Let's think step by step.To solve the problem, we need to determine the total number of apples picked by Jenna and her mother combined. We are given two pieces of information:\n",
      "\n",
      "1. Jenna's mother picked 20 apples.\n",
      "2. Jenna picked half as many apples as her mother.\n",
      "\n",
      "Let's break down the solution process:\n",
      "\n",
      "Step 1: Determine the number of apples Jenna's mother picked.\n",
      "- According to the information provided, Jenna's mother picked 20 apples.\n",
      "\n",
      "Step 2: Calculate the number of apples Jenna picked.\n",
      "- Since Jenna picked half as many apples as her mother, we need to find half of the mother's count.\n",
      "- Half of 20 apples is calculated by dividing 20 by 2, which gives us 10 apples.\n",
      "- Therefore, Jenna picked 10 apples.\n",
      "\n",
      "Step 3: Find the total number of apples they both picked.\n",
      "- To find the total, we add the number of apples picked by Jenna to the number of apples picked by her mother.\n",
      "- Adding Jenna's 10 apples to her mother's 20 apples gives us a total of 30 apples.\n",
      "\n",
      "Step 4: State the final answer.\n",
      "- The total number of apples picked by Jenna and her mother is 30.\n",
      "\n",
      "Therefore, the final answer to the question is \\boxed{30}.\n",
      "\n",
      "The answer is: 30.\n",
      "\n",
      "Intermetidate Response 2:\n",
      "\n",
      "If Jenna's mother picked 20 apples and Jenna picked half as many apples as her mother, then Jenna picked:\n",
      "\n",
      "\\[ \\frac{1}{2} \\times 20 = 10 \\text{ apples} \\]\n",
      "\n",
      "Together, they picked:\n",
      "\n",
      "\\[ 20 + 10 = 30 \\text{ apples} \\]\n",
      "\n",
      "So, Jenna and her mother picked a total of 30 apples.\n",
      "\n",
      "Intermetidate Response 3:\n",
      "\n",
      "Here's how to solve the problem:\n",
      "\n",
      "* **Jenna's apples:** Jenna picked half the amount her mom did, so she picked 20 / 2 = 10 apples.\n",
      "* **Total apples:** Together they picked 20 + 10 = 30 apples.\n",
      "\n",
      "\n",
      "**Answer:** They picked a total of 30 apples. \n",
      "\n",
      "\n",
      "Intermetidate Response 4:\n",
      "\n",
      "To find the total number of apples picked, we need to find out how many Jenna picked, then add that to the number her mom picked. \n",
      "\n",
      "Since Jenna picked half as many apples as her mom, and her mom picked 20 apples, Jenna picked 20 / 2 = 10 apples.\n",
      "\n",
      "So, Jenna's mom picked 20 apples and Jenna picked 10 apples. \n",
      "\n",
      "The total number of apples picked is 20 + 10 = 30 apples. The answer is 30.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, response in enumerate(intermediate_reponses):\n",
    "    print(f\"Intermetidate Response {i+1}:\\n\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer: To determine the total number of apples Jenna and her mother picked together, follow these steps:\n",
      "\n",
      "1. **Determine the number of apples Jenna's mother picked:**\n",
      "   - Jenna's mother picked **20 apples**.\n",
      "\n",
      "2. **Calculate the number of apples Jenna picked:**\n",
      "   - Jenna picked half as many apples as her mother.\n",
      "   - Half of 20 is \\( \\frac{1}{2} \\times 20 = 10 \\) apples.\n",
      "   - So, Jenna picked **10 apples**.\n",
      "\n",
      "3. **Find the total number of apples they both picked:**\n",
      "   - Add the number of apples picked by Jenna and her mother: \\( 20 + 10 = 30 \\) apples.\n",
      "\n",
      "**Final Answer:** Jenna and her mother picked a total of **30 apples**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrator Agent Workflow\n",
    "A simple snippet of the parallel orchestrator-worker agent workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, List\n",
    "\n",
    "ORCHESTRATOR_PROMPT = \"\"\"\n",
    "Analyze this task and break it down into 2-3 distinct approaches:\n",
    "\n",
    "Task: {task}\n",
    "\n",
    "Provide an Analysis:\n",
    "\n",
    "Explain your understanding of the task and which variations would be valuable.\n",
    "Focus on how each approach serves different aspects of the task.\n",
    "\n",
    "Along with the analysis, provide 2-3 approaches to tackle the task, each with a brief description:\n",
    "\n",
    "Formal style: Write technically and precisely, focusing on detailed specifications\n",
    "Conversational style: Write in a friendly and engaging way that connects with the reader\n",
    "Hybrid style: Tell a story that includes technical details, combining emotional elements with specifications\n",
    "\n",
    "Return only JSON output.\n",
    "\"\"\"\n",
    "\n",
    "WORKER_PROMPT = \"\"\"\n",
    "Generate content based on:\n",
    "Task: {original_task}\n",
    "Style: {task_type}\n",
    "Guidelines: {task_description}\n",
    "\n",
    "Return only your response:\n",
    "[Your content here, maintaining the specified style and fully addressing requirements.]\n",
    "\"\"\"\n",
    "\n",
    "task = \"\"\"Write a product description for a new eco-friendly water bottle.\n",
    "The target_audience is environmentally conscious millennials and key product features are: plastic-free, insulated, lifetime warranty\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(BaseModel):\n",
    "    type: Literal[\"formal\", \"conversational\", \"hybrid\"]\n",
    "    description: str\n",
    "\n",
    "class TaskList(BaseModel):\n",
    "    analysis: str\n",
    "    tasks: List[Task]  = Field(..., default_factory=list)\n",
    "\n",
    "async def orchestrator_workflow(task : str, orchestrator_prompt : str, worker_prompt : str): \n",
    "    \"\"\"Use a orchestrator model to break down a task into sub-tasks and then use worker models to generate and return responses.\"\"\"\n",
    "\n",
    "    # Use orchestrator model to break the task up into sub-tasks\n",
    "    orchestrator_response = JSON_llm(orchestrator_prompt.format(task=task), schema=TaskList)\n",
    " \n",
    "    # Parse orchestrator response\n",
    "    analysis = orchestrator_response[\"analysis\"]\n",
    "    tasks= orchestrator_response[\"tasks\"]\n",
    "\n",
    "    print(\"\\n=== ORCHESTRATOR OUTPUT ===\")\n",
    "    print(f\"\\nANALYSIS:\\n{analysis}\")\n",
    "    print(f\"\\nTASKS:\\n{json.dumps(tasks, indent=2)}\")\n",
    "\n",
    "    worker_model =  [\"meta-llama/Llama-3.3-70B-Instruct-Turbo\"]*len(tasks)\n",
    "\n",
    "    # Gather intermediate responses from worker models\n",
    "    return tasks , await asyncio.gather(*[run_llm_parallel(user_prompt=worker_prompt.format(original_task=task, task_type=task_info['type'], task_description=task_info['description']), model=model) for task_info, model in zip(tasks,worker_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ORCHESTRATOR OUTPUT ===\n",
      "\n",
      "ANALYSIS:\n",
      "The task requires writing a product description for an eco-friendly water bottle targeting environmentally conscious millennials. The key features to highlight are the plastic-free material, insulated design, and lifetime warranty. A valuable product description should effectively communicate these features while resonating with the target audience.\n",
      "\n",
      "TASKS:\n",
      "[\n",
      "  {\n",
      "    \"type\": \"formal\",\n",
      "    \"description\": \"Write a technically precise product description focusing on detailed specifications, such as the materials used, insulation technology, and warranty terms. This approach serves the task by providing a clear understanding of the product's features and benefits.\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"conversational\",\n",
      "    \"description\": \"Write a friendly and engaging product description that connects with the reader on an emotional level. This approach serves the task by building a relationship with the target audience and highlighting the product's eco-friendly aspects in a relatable way.\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"hybrid\",\n",
      "    \"description\": \"Tell a story that incorporates technical details, combining emotional elements with specifications. This approach serves the task by creating an immersive experience for the reader, highlighting the product's features, and emphasizing its eco-friendly benefits in a compelling narrative.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "task = \"\"\"Write a product description for a new eco-friendly water bottle. \n",
    "The target_audience is environmentally conscious millennials and key product features are: plastic-free, insulated, lifetime warranty\n",
    "\"\"\"\n",
    "\n",
    "tasks, worker_resp = await orchestrator_workflow(task, orchestrator_prompt=ORCHESTRATOR_PROMPT, worker_prompt=WORKER_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WORKER RESULT (formal) ===\n",
      "Introduction to the HydraGreen Water Bottle\n",
      "\n",
      "We are pleased to introduce the HydraGreen water bottle, a revolutionary, eco-friendly hydration solution designed specifically for environmentally conscious millennials. This premium product boasts a plastic-free construction, advanced insulation technology, and a comprehensive lifetime warranty, ensuring a superior user experience while minimizing its ecological footprint.\n",
      "\n",
      "Materials and Construction\n",
      "\n",
      "The HydraGreen water bottle is crafted from high-quality, BPA-free stainless steel (18/8 food-grade) and features a durable, non-toxic silicone sleeve. The bottle's body is constructed using a proprietary double-walled insulation process, which provides exceptional thermal retention while maintaining a slim, ergonomic design. The lid is manufactured from a sustainable, plant-based polymer, further reducing the product's reliance on petroleum-derived materials.\n",
      "\n",
      "Insulation Technology\n",
      "\n",
      "The HydraGreen water bottle incorporates our patented ThermaLock insulation system, which leverages the principles of vacuum insulation to minimize heat transfer. This advanced technology enables the bottle to maintain temperatures for an extended period, keeping beverages hot for up to 12 hours or cold for up to 24 hours. The ThermaLock system consists of a evacuated insulation space, a high-performance insulation material, and a precision-engineered lid seal, ensuring optimal thermal performance.\n",
      "\n",
      "Lifetime Warranty\n",
      "\n",
      "The HydraGreen water bottle is backed by a comprehensive lifetime warranty, providing users with unparalleled peace of mind. The warranty covers all defects in materials and workmanship, including the insulation system, lid, and bottle body. In the unlikely event of a defect, our dedicated customer service team will promptly replace or repair the product, free of charge. The warranty is valid for the lifetime of the product, with no expiration date or limitations on the number of claims.\n",
      "\n",
      "Specifications\n",
      "\n",
      "- Material: 18/8 food-grade stainless steel, BPA-free silicone, and plant-based polymer\n",
      "- Insulation: Patented ThermaLock vacuum insulation system\n",
      "- Capacity: 27 oz (800 ml)\n",
      "- Dimensions: 10.5 in (26.7 cm) x 3.5 in (8.9 cm)\n",
      "- Weight: 13.5 oz (383 g)\n",
      "- Temperature range: -20°C to 100°C (-4°F to 212°F)\n",
      "- Warranty: Lifetime warranty, covering defects in materials and workmanship\n",
      "\n",
      "Conclusion\n",
      "\n",
      "The HydraGreen water bottle is an exemplary model of innovative, eco-friendly design, combining exceptional performance, durability, and sustainability. With its plastic-free construction, advanced insulation technology, and comprehensive lifetime warranty, this product is poised to become an indispensable companion for environmentally conscious millennials seeking a reliable, high-quality hydration solution.\n",
      "\n",
      "\n",
      "=== WORKER RESULT (conversational) ===\n",
      "Meet your new favorite companion - our eco-friendly water bottle that's as passionate about the planet as you are. As someone who cares deeply about the environment, you're probably tired of using plastic water bottles that end up in landfills and oceans. That's why we created a game-changing alternative that's not only better for the Earth, but also keeps your drinks hot or cold for hours.\n",
      "\n",
      "Our bottle is made from sustainable, plastic-free materials that are designed to last a lifetime. And we mean that literally - we're so confident in the quality of our product that we're backing it with a lifetime warranty. Whether you're hiking, biking, or just running errands, our insulated bottle will keep your drinks at the perfect temperature, and its durable design will withstand even the toughest adventures.\n",
      "\n",
      "But here's the thing: this bottle is about so much more than just keeping your drinks hot or cold. It's about making a statement. It's about saying that you care about the planet, and you're willing to take small steps every day to reduce your impact. By choosing our eco-friendly water bottle, you're joining a community of like-minded individuals who are passionate about creating a more sustainable future.\n",
      "\n",
      "So why settle for a flimsy, plastic water bottle that's just going to end up in a landfill? Choose a bottle that aligns with your values, and that will be your trusted sidekick for years to come. Join the movement towards a more sustainable future, and experience the difference that our eco-friendly water bottle can make.\n",
      "\n",
      "\n",
      "=== WORKER RESULT (hybrid) ===\n",
      "Imagine yourself hiking through a lush forest, the warm sun beating down on your skin, and the sound of a gentle stream babbling in the distance. As you walk, you can't help but feel a deep connection to the natural world around you. But with that connection comes a sense of responsibility – a responsibility to protect the planet and preserve its beauty for future generations.\n",
      "\n",
      "That's where our new eco-friendly water bottle comes in. Made from 100% plastic-free materials, this innovative bottle is not only a stylish accessory but also a powerful tool in the fight against plastic waste. The insulated design keeps your drinks hot or cold for hours, whether you're sipping coffee on a chilly morning or enjoying a refreshing glass of water on a sweltering summer day.\n",
      "\n",
      "But what really sets our bottle apart is its lifetime warranty. We're so confident in the quality and durability of our product that we're willing to stand behind it for life. That means you can enjoy your favorite beverages without worrying about the environmental impact of constantly replacing disposable bottles. And with our bottle's sleek, modern design, you'll be proud to show it off to friends and family.\n",
      "\n",
      "But the story doesn't end there. Every time you use our eco-friendly water bottle, you're making a statement. You're saying that you care about the planet and its inhabitants, and that you're willing to take action to protect them. You're joining a community of like-minded individuals who are committed to reducing their plastic footprint and living a more sustainable lifestyle.\n",
      "\n",
      "So why settle for a boring, disposable water bottle when you can have a stylish, eco-friendly one that makes a real difference? Our plastic-free, insulated bottle is the perfect choice for anyone who wants to stay hydrated while also doing their part for the planet. With its lifetime warranty and sleek design, it's an investment that will pay off for years to come.\n",
      "\n",
      "Technical specifications:\n",
      "\n",
      "* Material: 100% plastic-free, BPA-free, and phthalate-free\n",
      "* Insulation: Double-walled insulation keeps drinks hot or cold for hours\n",
      "* Warranty: Lifetime warranty against defects and damage\n",
      "* Capacity: 27 oz (800 ml)\n",
      "* Dimensions: 10.5 inches (26.7 cm) tall, 3.5 inches (8.9 cm) wide\n",
      "* Weight: 1.2 pounds (0.5 kg)\n",
      "\n",
      "Join the movement towards a more sustainable future. Choose our eco-friendly water bottle and experience the perfect blend of style, functionality, and environmental responsibility.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task_info, response in zip(tasks, worker_resp):\n",
    "    print(f\"\\n=== WORKER RESULT ({task_info['type']}) ===\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop Optimizer Agent Workflow\n",
    "A simple snippet of looping generator-evaluator workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"\n",
    "Implement a Stack with:\n",
    "1. push(x)\n",
    "2. pop()\n",
    "3. getMin()\n",
    "All operations should be O(1).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_PROMPT = \"\"\"\n",
    "Your goal is to complete the task based on <user input>. If there are feedback \n",
    "from your previous generations, you should reflect on them to improve your solution\n",
    "\n",
    "Output your answer concisely in the following format: \n",
    "\n",
    "Thoughts:\n",
    "[Your understanding of the task and feedback and how you plan to improve]\n",
    "\n",
    "Response:\n",
    "[Your code implementation here]\n",
    "\"\"\"\n",
    "\n",
    "def generate(task: str, generator_prompt: str, context: str = \"\") -> tuple[str, str]:\n",
    "    \"\"\"Generate and improve a solution based on feedback.\"\"\"\n",
    "    full_prompt = f\"{generator_prompt}\\n{context}\\nTask: {task}\" if context else f\"{generator_prompt}\\nTask: {task}\"\n",
    "\n",
    "    response = run_llm(full_prompt, model=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "    \n",
    "    print(\"\\n=== GENERATION START ===\")\n",
    "    print(f\"Output:\\n{response}\\n\")\n",
    "    print(\"=== GENERATION END ===\\n\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "EVALUATOR_PROMPT = \"\"\"\n",
    "Evaluate this following code implementation for:\n",
    "1. code correctness\n",
    "2. time complexity\n",
    "3. style and best practices\n",
    "\n",
    "You should be evaluating only and not attemping to solve the task.\n",
    "\n",
    "Only output \"PASS\" if all criteria are met and you have no further suggestions for improvements.\n",
    "\n",
    "Provide detailed feedback if there are areas that need improvement. You should specify what needs improvement and why.\n",
    "\n",
    "Only output JSON.\n",
    "\"\"\"\n",
    "\n",
    "def evaluate(task : str, evaluator_prompt : str, generated_content: str, schema) -> tuple[str, str]:\n",
    "    \"\"\"Evaluate if a solution meets requirements.\"\"\"\n",
    "    full_prompt = f\"{evaluator_prompt}\\nOriginal task: {task}\\nContent to evaluate: {generated_content}\"\n",
    "    \n",
    "    response = JSON_llm(full_prompt, schema)\n",
    "    \n",
    "    evaluation = response[\"evaluation\"]\n",
    "    feedback = response[\"feedback\"]\n",
    "\n",
    "    print(\"=== EVALUATION START ===\")\n",
    "    print(f\"Status: {evaluation}\")\n",
    "    print(f\"Feedback: {feedback}\")\n",
    "    print(\"=== EVALUATION END ===\\n\")\n",
    "\n",
    "    return evaluation, feedback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_workflow(task: str, evaluator_prompt: str, generator_prompt: str) -> tuple[str, list[dict]]:\n",
    "    \"\"\"Keep generating and evaluating until the evaluator passes the last generated response.\"\"\"\n",
    "    # Store previous responses from generator\n",
    "    memory = []\n",
    "    \n",
    "    # Generate initial response\n",
    "    response = generate(task, generator_prompt)\n",
    "    memory.append(response)\n",
    "\n",
    "    #Build a schema for the evaluation\n",
    "    class Evaluation(BaseModel):\n",
    "        evaluation: Literal[\"PASS\", \"NEEDS_IMPROVEMENT\", \"FAIL\"]\n",
    "        feedback: str\n",
    "\n",
    "    # While the generated response is not passing, keep generating and evaluating\n",
    "    while True:\n",
    "        evaluation, feedback = evaluate(task, evaluator_prompt, response, Evaluation)\n",
    "        # Terminating condition\n",
    "        if evaluation == \"PASS\":\n",
    "            return response\n",
    "        \n",
    "        # Add current response and feedback to context and generate a new response\n",
    "        context = \"\\n\".join([\n",
    "            \"Previous attempts:\",\n",
    "            *[f\"- {m}\" for m in memory],\n",
    "            f\"\\nFeedback: {feedback}\"\n",
    "        ])\n",
    "        \n",
    "        response = generate(generator_prompt, task, context)\n",
    "        memory.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GENERATION START ===\n",
      "Output:\n",
      "Thoughts:\n",
      "To implement a stack with the operations `push(x)`, `pop()`, and `getMin()` all in O(1) time complexity, we can use two stacks. One stack will store the actual stack elements, and the other will store the minimum values. The minimum stack will help us keep track of the minimum element efficiently.\n",
      "\n",
      "Response:\n",
      "```python\n",
      "class MinStack:\n",
      "    def __init__(self):\n",
      "        self.stack = []\n",
      "        self.min_stack = []\n",
      "\n",
      "    def push(self, x: int) -> None:\n",
      "        self.stack.append(x)\n",
      "        if not self.min_stack or x <= self.min_stack[-1]:\n",
      "            self.min_stack.append(x)\n",
      "\n",
      "    def pop(self) -> None:\n",
      "        if self.stack:\n",
      "            x = self.stack.pop()\n",
      "            if x == self.min_stack[-1]:\n",
      "                self.min_stack.pop()\n",
      "\n",
      "    def top(self) -> int:\n",
      "        if self.stack:\n",
      "            return self.stack[-1]\n",
      "        raise IndexError(\"Stack is empty\")\n",
      "\n",
      "    def getMin(self) -> int:\n",
      "        if self.min_stack:\n",
      "            return self.min_stack[-1]\n",
      "        raise IndexError(\"Stack is empty\")\n",
      "\n",
      "# Example usage:\n",
      "# min_stack = MinStack()\n",
      "# min_stack.push(-2)\n",
      "# min_stack.push(0)\n",
      "# min_stack.push(-3)\n",
      "# print(min_stack.getMin())  # Returns -3\n",
      "# min_stack.pop()\n",
      "# print(min_stack.top())     # Returns 0\n",
      "# print(min_stack.getMin())  # Returns -2\n",
      "```\n",
      "\n",
      "This implementation ensures that all operations (`push`, `pop`, and `getMin`) are performed in O(1) time complexity. The `min_stack` keeps track of the minimum elements, allowing us to retrieve the minimum element in constant time.\n",
      "\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "Status: FAIL\n",
      "Feedback: The code is mostly correct, but there are a few issues and areas for improvement.\n",
      "=== EVALUATION END ===\n",
      "\n",
      "\n",
      "=== GENERATION START ===\n",
      "Output:\n",
      "Thoughts:\n",
      "The previous implementation is mostly correct and efficient, ensuring that all operations (`push`, `pop`, and `getMin()`) are performed in O(1) time complexity. However, we can make some improvements to enhance readability and handle edge cases more gracefully. Specifically, we should ensure that the `top()` method does not raise an exception when the stack is empty, as it is not a typical expectation for a stack's `top()` method to raise an exception in all cases. Instead, we can return `None` or a suitable message indicating that the stack is empty.\n",
      "\n",
      "Response:\n",
      "```python\n",
      "class MinStack:\n",
      "    def __init__(self):\n",
      "        self.stack = []\n",
      "        self.min_stack = []\n",
      "\n",
      "    def push(self, x: int) -> None:\n",
      "        self.stack.append(x)\n",
      "        if not self.min_stack or x <= self.min_stack[-1]:\n",
      "            self.min_stack.append(x)\n",
      "\n",
      "    def pop(self) -> None:\n",
      "        if self.stack:\n",
      "            x = self.stack.pop()\n",
      "            if x == self.min_stack[-1]:\n",
      "                self.min_stack.pop()\n",
      "\n",
      "    def top(self) -> int:\n",
      "        if self.stack:\n",
      "            return self.stack[-1]\n",
      "        return None  # Return None if the stack is empty\n",
      "\n",
      "    def getMin(self) -> int:\n",
      "        if self.min_stack:\n",
      "            return self.min_stack[-1]\n",
      "        return None  # Return None if the stack is empty\n",
      "\n",
      "# Example usage:\n",
      "# min_stack = MinStack()\n",
      "# min_stack.push(-2)\n",
      "# min_stack.push(0)\n",
      "# min_stack.push(-3)\n",
      "# print(min_stack.getMin())  # Returns -3\n",
      "# min_stack.pop()\n",
      "# print(min_stack.top())     # Returns 0\n",
      "# print(min_stack.getMin())  # Returns -2\n",
      "```\n",
      "\n",
      "This updated implementation handles empty stack scenarios more gracefully by returning `None` from the `top()` and `getMin()` methods when the stack is empty, rather than raising an exception.\n",
      "\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "Status: FAIL\n",
      "Feedback: Code Correctness: PASS, Time Complexity: PASS, Style and Best Practices: FAIL, improvements needed: \n",
      "=== EVALUATION END ===\n",
      "\n",
      "\n",
      "=== GENERATION START ===\n",
      "Output:\n",
      "Thoughts:\n",
      "The current implementation is correct in terms of functionality and time complexity. However, the feedback suggests improving style and best practices. Specifically, we should ensure consistent return types and handle edge cases gracefully. Returning `None` for empty stack scenarios is a good practice, but we can also add docstrings to improve code readability and maintainability.\n",
      "\n",
      "Response:\n",
      "```python\n",
      "class MinStack:\n",
      "    def __init__(self):\n",
      "        \"\"\"Initialize the stack and the minimum stack.\"\"\"\n",
      "        self.stack = []\n",
      "        self.min_stack = []\n",
      "\n",
      "    def push(self, x: int) -> None:\n",
      "        \"\"\"Push element x onto the stack.\"\"\"\n",
      "        self.stack.append(x)\n",
      "        if not self.min_stack or x <= self.min_stack[-1]:\n",
      "            self.min_stack.append(x)\n",
      "\n",
      "    def pop(self) -> None:\n",
      "        \"\"\"Remove the element on top of the stack.\"\"\"\n",
      "        if self.stack:\n",
      "            x = self.stack.pop()\n",
      "            if x == self.min_stack[-1]:\n",
      "                self.min_stack.pop()\n",
      "\n",
      "    def top(self) -> int:\n",
      "        \"\"\"Get the top element of the stack.\"\"\"\n",
      "        return self.stack[-1] if self.stack else None\n",
      "\n",
      "    def getMin(self) -> int:\n",
      "        \"\"\"Retrieve the minimum element in the stack.\"\"\"\n",
      "        return self.min_stack[-1] if self.min_stack else None\n",
      "\n",
      "# Example usage:\n",
      "# min_stack = MinStack()\n",
      "# min_stack.push(-2)\n",
      "# min_stack.push(0)\n",
      "# min_stack.push(-3)\n",
      "# print(min_stack.getMin())  # Returns -3\n",
      "# min_stack.pop()\n",
      "# print(min_stack.top())     # Returns 0\n",
      "# print(min_stack.getMin())  # Returns -2\n",
      "```\n",
      "\n",
      "This updated implementation includes docstrings for each method to improve code readability and maintainability, while still adhering to the requirement of handling empty stack scenarios gracefully by returning `None`.\n",
      "\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "Status: FAIL\n",
      "Feedback: code_correctness: PASS, time_complexity: PASS, style_and_best_practices: FAIL, suggestions: {\"1. Error Handling\": \"The current implementation does not handle potential errors that may occur during execution. For instance, the pop method does not check if the stack is empty before attempting to remove an element, which can lead to an IndexError. It is essential to add error handling mechanisms to ensure the code behaves as expected in different scenarios.\", \"2. Type Hints\": \"Although type hints are provided for the push method, they are missing for the pop, top, and getMin methods. Consistent use of type hints can improve code readability and maintainability.\", \"3. Docstrings\": \"While docstrings are provided for each method, they do not follow the Google Python Style Guide, which is a widely accepted standard for Python documentation. It is recommended to adhere to this style guide for consistency and readability.\", \"4. Redundant Code\": \"The getMin method checks if the min_stack is empty before returning the minimum element. However, this check is redundant since the push method ensures that the min_stack is updated accordingly. The getMin method can be simplified by removing this check.\", \"5. Method Naming\": \"The method names are clear and descriptive. However, the name 'top' is not explicitly mentioned in the problem statement. Consider renaming it to something more descriptive, such as 'get_top_element'.\", \"6. Return Type Consistency\": \"The return types of the methods are not consistent. The push and pop methods return None, while the top and getMin methods return the top element and the minimum element, respectively, or None if the stack is empty. It is essential to maintain consistent return types throughout the class.\", \"7. Example Usage\": \"The example usage is not part of the class implementation and should be separated from the class definition. Consider adding it as a separate test suite or in a __main__ block.\"}\n",
      "=== EVALUATION END ===\n",
      "\n",
      "\n",
      "=== GENERATION START ===\n",
      "Output:\n",
      "Thoughts:\n",
      "The feedback highlights several areas for improvement in the `MinStack` implementation, including error handling, type hints, docstring formatting, method naming, return type consistency, and example usage separation. I will address these points to enhance the code's robustness and adherence to best practices.\n",
      "\n",
      "1. **Error Handling**: Ensure that the `pop` method does not raise an `IndexError` if the stack is empty.\n",
      "2. **Type Hints**: Add type hints to all methods for better readability and maintainability.\n",
      "3. **Docstrings**: Follow the Google Python Style Guide for consistency.\n",
      "4. **Redundant Code**: Simplify the `getMin` method by removing unnecessary checks.\n",
      "5. **Method Naming**: Rename `top` to `get_top_element` for clarity.\n",
      "6. **Return Type Consistency**: Ensure that the return types are consistent across methods.\n",
      "7. **Example Usage**: Separate the example usage into a `__main__` block.\n",
      "\n",
      "Response:\n",
      "```python\n",
      "class MinStack:\n",
      "    def __init__(self):\n",
      "        \"\"\"Initialize the stack and the minimum stack.\"\"\"\n",
      "        self.stack = []\n",
      "        self.min_stack = []\n",
      "\n",
      "    def push(self, x: int) -> None:\n",
      "        \"\"\"Push element x onto the stack.\n",
      "\n",
      "        Args:\n",
      "            x (int): The element to be pushed onto the stack.\n",
      "        \"\"\"\n",
      "        self.stack.append(x)\n",
      "        if not self.min_stack or x <= self.min_stack[-1]:\n",
      "            self.min_stack.append(x)\n",
      "\n",
      "    def pop(self) -> None:\n",
      "        \"\"\"Remove the element on top of the stack.\n",
      "\n",
      "        Raises:\n",
      "            IndexError: If the stack is empty.\n",
      "        \"\"\"\n",
      "        if not self.stack:\n",
      "            raise IndexError(\"pop from empty stack\")\n",
      "        x = self.stack.pop()\n",
      "        if x == self.min_stack[-1]:\n",
      "            self.min_stack.pop()\n",
      "\n",
      "    def get_top_element(self) -> int:\n",
      "        \"\"\"Get the top element of the stack.\n",
      "\n",
      "        Returns:\n",
      "            int: The top element of the stack.\n",
      "\n",
      "        Raises:\n",
      "            IndexError: If the stack is empty.\n",
      "        \"\"\"\n",
      "        if not self.stack:\n",
      "            raise IndexError(\"top from empty stack\")\n",
      "        return self.stack[-1]\n",
      "\n",
      "    def get_min(self) -> int:\n",
      "        \"\"\"Retrieve the minimum element in the stack.\n",
      "\n",
      "        Returns:\n",
      "            int: The minimum element in the stack.\n",
      "\n",
      "        Raises:\n",
      "            IndexError: If the stack is empty.\n",
      "        \"\"\"\n",
      "        if not self.min_stack:\n",
      "            raise IndexError(\"get_min from empty stack\")\n",
      "        return self.min_stack[-1]\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # Example usage:\n",
      "    min_stack = MinStack()\n",
      "    min_stack.push(-2)\n",
      "    min_stack.push(0)\n",
      "    min_stack.push(-3)\n",
      "    print(min_stack.get_min())  # Returns -3\n",
      "    min_stack.pop()\n",
      "    print(min_stack.get_top_element())     # Returns 0\n",
      "    print(min_stack.get_min())  # Returns -2\n",
      "```\n",
      "\n",
      "This updated implementation addresses all the feedback points, ensuring that the code is robust, follows best practices, and is easy to understand and maintain.\n",
      "\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "Status: FAIL\n",
      "Feedback: The provided implementation meets most of the criteria but requires some adjustments for improvement.\n",
      "=== EVALUATION END ===\n",
      "\n",
      "\n",
      "=== GENERATION START ===\n",
      "Output:\n",
      "Thoughts:\n",
      "The previous implementation is mostly correct but can be improved by handling empty stack scenarios more gracefully, ensuring consistent return types, and providing clear documentation. Specifically, we should return `None` instead of raising an `IndexError` for empty stack operations, and we should add type hints and follow a consistent docstring style.\n",
      "\n",
      "Response:\n",
      "```python\n",
      "class MinStack:\n",
      "    def __init__(self):\n",
      "        \"\"\"Initialize the stack and the minimum stack.\"\"\"\n",
      "        self.stack = []\n",
      "        self.min_stack = []\n",
      "\n",
      "    def push(self, x: int) -> None:\n",
      "        \"\"\"Push element x onto the stack.\n",
      "\n",
      "        Args:\n",
      "            x (int): The element to be pushed onto the stack.\n",
      "        \"\"\"\n",
      "        self.stack.append(x)\n",
      "        if not self.min_stack or x <= self.min_stack[-1]:\n",
      "            self.min_stack.append(x)\n",
      "\n",
      "    def pop(self) -> None:\n",
      "        \"\"\"Remove the element on top of the stack.\"\"\"\n",
      "        if self.stack:\n",
      "            x = self.stack.pop()\n",
      "            if x == self.min_stack[-1]:\n",
      "                self.min_stack.pop()\n",
      "\n",
      "    def top(self) -> int:\n",
      "        \"\"\"Get the top element of the stack.\n",
      "\n",
      "        Returns:\n",
      "            int: The top element of the stack, or None if the stack is empty.\n",
      "        \"\"\"\n",
      "        return self.stack[-1] if self.stack else None\n",
      "\n",
      "    def getMin(self) -> int:\n",
      "        \"\"\"Retrieve the minimum element in the stack.\n",
      "\n",
      "        Returns:\n",
      "            int: The minimum element in the stack, or None if the stack is empty.\n",
      "        \"\"\"\n",
      "        return self.min_stack[-1] if self.min_stack else None\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # Example usage:\n",
      "    min_stack = MinStack()\n",
      "    min_stack.push(-2)\n",
      "    min_stack.push(0)\n",
      "    min_stack.push(-3)\n",
      "    print(min_stack.getMin())  # Returns -3\n",
      "    min_stack.pop()\n",
      "    print(min_stack.top())     # Returns 0\n",
      "    print(min_stack.getMin())  # Returns -2\n",
      "```\n",
      "\n",
      "This implementation ensures that all operations (`push`, `pop`, `top`, and `getMin`) are performed in O(1) time complexity. It handles empty stack scenarios gracefully by returning `None` and includes clear documentation with type hints and docstrings.\n",
      "\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "Status: FAIL\n",
      "Feedback: Code correctness, style, and best practices have been improved but still have room for improvement. Here are some suggestions:\n",
      "=== EVALUATION END ===\n",
      "\n",
      "\n",
      "=== GENERATION START ===\n",
      "Output:\n",
      "Thoughts:\n",
      "The current implementation is mostly correct and handles edge cases by returning `None` for empty stack scenarios. However, we can further improve the code by:\n",
      "1. Ensuring consistent method naming (e.g., `getMin` to `get_min` for consistency with `get_top_element`).\n",
      "2. Adding type hints and docstrings for better readability and maintainability.\n",
      "3. Ensuring that the implementation adheres to best practices by handling empty stack scenarios gracefully and providing clear documentation.\n",
      "\n",
      "Response:\n",
      "```python\n",
      "class MinStack:\n",
      "    def __init__(self):\n",
      "        \"\"\"Initialize the stack and the minimum stack.\"\"\"\n",
      "        self.stack = []\n",
      "        self.min_stack = []\n",
      "\n",
      "    def push(self, x: int) -> None:\n",
      "        \"\"\"Push element x onto the stack.\n",
      "\n",
      "        Args:\n",
      "            x (int): The element to be pushed onto the stack.\n",
      "        \"\"\"\n",
      "        self.stack.append(x)\n",
      "        if not self.min_stack or x <= self.min_stack[-1]:\n",
      "            self.min_stack.append(x)\n",
      "\n",
      "    def pop(self) -> None:\n",
      "        \"\"\"Remove the element on top of the stack.\"\"\"\n",
      "        if self.stack:\n",
      "            x = self.stack.pop()\n",
      "            if x == self.min_stack[-1]:\n",
      "                self.min_stack.pop()\n",
      "\n",
      "    def get_top_element(self) -> int:\n",
      "        \"\"\"Get the top element of the stack.\n",
      "\n",
      "        Returns:\n",
      "            int: The top element of the stack, or None if the stack is empty.\n",
      "        \"\"\"\n",
      "        return self.stack[-1] if self.stack else None\n",
      "\n",
      "    def get_min(self) -> int:\n",
      "        \"\"\"Retrieve the minimum element in the stack.\n",
      "\n",
      "        Returns:\n",
      "            int: The minimum element in the stack, or None if the stack is empty.\n",
      "        \"\"\"\n",
      "        return self.min_stack[-1] if self.min_stack else None\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # Example usage:\n",
      "    min_stack = MinStack()\n",
      "    min_stack.push(-2)\n",
      "    min_stack.push(0)\n",
      "    min_stack.push(-3)\n",
      "    print(min_stack.get_min())  # Returns -3\n",
      "    min_stack.pop()\n",
      "    print(min_stack.get_top_element())     # Returns 0\n",
      "    print(min_stack.get_min())  # Returns -2\n",
      "```\n",
      "\n",
      "This implementation ensures that all operations (`push`, `pop`, `get_top_element`, and `get_min`) are performed in O(1) time complexity. It handles empty stack scenarios gracefully by returning `None` and includes clear documentation with type hints and docstrings. The method naming is consistent and follows best practices.\n",
      "\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "Status: PASS\n",
      "Feedback: No suggestions for improvement.\n",
      "=== EVALUATION END ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Thoughts:\\nThe current implementation is mostly correct and handles edge cases by returning `None` for empty stack scenarios. However, we can further improve the code by:\\n1. Ensuring consistent method naming (e.g., `getMin` to `get_min` for consistency with `get_top_element`).\\n2. Adding type hints and docstrings for better readability and maintainability.\\n3. Ensuring that the implementation adheres to best practices by handling empty stack scenarios gracefully and providing clear documentation.\\n\\nResponse:\\n```python\\nclass MinStack:\\n    def __init__(self):\\n        \"\"\"Initialize the stack and the minimum stack.\"\"\"\\n        self.stack = []\\n        self.min_stack = []\\n\\n    def push(self, x: int) -> None:\\n        \"\"\"Push element x onto the stack.\\n\\n        Args:\\n            x (int): The element to be pushed onto the stack.\\n        \"\"\"\\n        self.stack.append(x)\\n        if not self.min_stack or x <= self.min_stack[-1]:\\n            self.min_stack.append(x)\\n\\n    def pop(self) -> None:\\n        \"\"\"Remove the element on top of the stack.\"\"\"\\n        if self.stack:\\n            x = self.stack.pop()\\n            if x == self.min_stack[-1]:\\n                self.min_stack.pop()\\n\\n    def get_top_element(self) -> int:\\n        \"\"\"Get the top element of the stack.\\n\\n        Returns:\\n            int: The top element of the stack, or None if the stack is empty.\\n        \"\"\"\\n        return self.stack[-1] if self.stack else None\\n\\n    def get_min(self) -> int:\\n        \"\"\"Retrieve the minimum element in the stack.\\n\\n        Returns:\\n            int: The minimum element in the stack, or None if the stack is empty.\\n        \"\"\"\\n        return self.min_stack[-1] if self.min_stack else None\\n\\nif __name__ == \"__main__\":\\n    # Example usage:\\n    min_stack = MinStack()\\n    min_stack.push(-2)\\n    min_stack.push(0)\\n    min_stack.push(-3)\\n    print(min_stack.get_min())  # Returns -3\\n    min_stack.pop()\\n    print(min_stack.get_top_element())     # Returns 0\\n    print(min_stack.get_min())  # Returns -2\\n```\\n\\nThis implementation ensures that all operations (`push`, `pop`, `get_top_element`, and `get_min`) are performed in O(1) time complexity. It handles empty stack scenarios gracefully by returning `None` and includes clear documentation with type hints and docstrings. The method naming is consistent and follows best practices.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loop_workflow(task, EVALUATOR_PROMPT, GENERATOR_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
