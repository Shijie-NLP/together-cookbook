{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "73b0444b",
      "metadata": {
        "id": "73b0444b"
      },
      "source": [
        "# Using Toolhouse for Tool Use on Together.AI infrastructure\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/orliesaurus/together-cookbook/blob/toolhouse-tool-use-cookbook/Tool_use_with_Toolhouse.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "626a645c",
      "metadata": {
        "id": "626a645c"
      },
      "source": [
        "This Colab notebook leverages Together's powerful AI models and Toolhouse's streamlined function calling toolkit to efficiently execute complex tasks and generate creative content.\n",
        "\n",
        "[Toolhouse](https://app.toolhouse.ai/) is the first complete infrastructure platform for building, running and managing tool use.\n",
        "\n",
        "With Toolhouse, you can equip your LLM with extra skills (also known as tools).\n",
        "\n",
        "Some of the most popular tools include:\n",
        "- scraping data from the web or social media\n",
        "- generate images\n",
        "- compile or execute code and return the values\n",
        "\n",
        "This cookbook demonstrates how you can **equip LLMs running on** [Together.ai](https://together.ai/) - **with tools**, without the need for your to code or prompt these tools.\n",
        "\n",
        "In this short demo, we'll show how we can equip an LLM to generate images from real-data.\n",
        "\n",
        "We'll use Toolhouse with the model tagged `meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo` and hosted on Together's infrastructure.\n",
        "\n",
        "This model is fine-tuned for effective and precise tool use."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "002c27a7",
      "metadata": {
        "id": "002c27a7"
      },
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's install the dependencies.\n",
        "We will use the OpenAI SDK which is compatible with Together's API - as long as we set the base_url.\n",
        "\n",
        "We're also installing Toolhouse's Python SDK to access pre-built, pre-hosted tools."
      ],
      "metadata": {
        "id": "M11wAhQR2JLu"
      },
      "id": "M11wAhQR2JLu"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install toolhouse openai"
      ],
      "metadata": {
        "id": "LaMYsh-3580q",
        "collapsed": true
      },
      "id": "LaMYsh-3580q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d237c86c",
      "metadata": {
        "id": "d237c86c"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from toolhouse import Toolhouse"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "458ace84",
      "metadata": {
        "id": "458ace84"
      },
      "source": [
        "To integrate Together and Toolhouse, you'll need to set up two environment variables: `TOGETHER_API_KEY` and `TOOLHOUSE_API_KEY`. Follow these steps to obtain your API keys:\n",
        "\n",
        "* **Together API Key**: Get your key by visiting the [Together Console](https://api.together.ai/).\n",
        "* **Toolhouse API Key**: Sign up for Toolhouse using [this link](https://join.toolhouse.ai) to receive $150 in credits. You will receive your API key as part of the onboarding step, and you can always, navigate to the [Toolhouse API Keys page](https://app.toolhouse.ai/settings/api-keys) to create and get an API key.\n",
        "* **Install the X tool**: In your [Toolhouse dashboard](https://app.toolhouse.ai), click Install next to the \"Search X\" tool ([direct link](https://app.toolhouse.ai/store/search_x)).\n",
        "* **Install the Image Generation tool**: In your [Toolhouse dashboard](https://app.toolhouse.ai), click Install next to the \"Image Generation\" tool ([direct link](https://app.toolhouse.ai/store/image_generation_flux)).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up keys in Colab\n",
        "\n",
        "Once you have signe-up and got both API keys, set them as environment variables to start using Together with Toolhouse.\n",
        "\n",
        "To do that click on the Key icon in the left menu on Colab and click **\"Add new secret\".**\n",
        "\n",
        "Create 2 keys and make sure their **name** is spelled like below:\n",
        "```\n",
        "TOGETHER_API_KEY\n",
        "TOOLHOUSE_API_KEY\n",
        "```\n",
        "\n",
        "Enter the values for each key and switch on the **allow note book access\"**"
      ],
      "metadata": {
        "id": "dEPpWhup3Fva"
      },
      "id": "dEPpWhup3Fva"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Setting up the SDKs\n",
        "\n",
        "Since we've imported the SDKs already, let's initialize them and authenticate ourselves using the API keys we just set.\n",
        "\n",
        "If you want to check again, your API keys should be now in the left sidebar of Google Colab after clicking the key icon."
      ],
      "metadata": {
        "id": "_kblVg3pXlMy"
      },
      "id": "_kblVg3pXlMy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf32deff",
      "metadata": {
        "id": "bf32deff"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "client = OpenAI(base_url = \"https://api.together.xyz/v1\", api_key=userdata.get(\"TOGETHER_API_KEY\"))\n",
        "\n",
        "th = Toolhouse(api_key=userdata.get('TOOLHOUSE_API_KEY'), provider=\"openai\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32220c3f",
      "metadata": {
        "id": "32220c3f"
      },
      "source": [
        "We will use a Meta model specificially the Instruct flavor:\n",
        "- [meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\n",
        " model](https://docs.together.ai/docs/function-calling#supported-models).\n",
        "\n",
        "This model is based on the Meta 3.1 70B model and was fine-tuned for tool use and function calling:\n",
        "\n",
        "We can test that we can call the model hosted on Together by running the cell below. It should output some info about NYC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8771ddd7",
      "metadata": {
        "id": "8771ddd7"
      },
      "outputs": [],
      "source": [
        "MODEL = \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"tell me about new york\"}],\n",
        ")\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89ad14fd",
      "metadata": {
        "id": "89ad14fd"
      },
      "source": [
        "Likewise, you can use the `th.get_tools()` function to display all of the Toolhouse tools you have installed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d814c0bf",
      "metadata": {
        "id": "d814c0bf"
      },
      "outputs": [],
      "source": [
        "print('TOOLS AVAILABLE:')\n",
        "for tool in th.get_tools():\n",
        "    print(f\"Name: {tool['function']['name']}\")\n",
        "    print(f\"Type: {tool['type']}\")\n",
        "    print(f\"Description: {tool['function']['description']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b2b413",
      "metadata": {
        "id": "b5b2b413"
      },
      "source": [
        "For this demo, we will be using two tools from Toolhouse's store.\n",
        "- Search X\n",
        "- Image generation\n",
        "\n",
        "The first tool lets you search the X social network and the second tool lets you generate an image from a prompt and returns it to the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd565fc7",
      "metadata": {
        "id": "cd565fc7"
      },
      "source": [
        "# 3. Configure Tool Calls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09bf2fec",
      "metadata": {
        "id": "09bf2fec"
      },
      "source": [
        "First we'll configure the user prompt to the LLM.\n",
        "This is a basic python list with a dictionary within it that takes `role` and a `content`.\n",
        "\n",
        "We set `role` to `user` because this is a command that we, the user, want the LLM to understand.\n",
        "\n",
        "Doing so will allow us to give a specific command to the LLM. The LLM will look at what tools we have available and attempt to leverage them to achieve the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b457ee7",
      "metadata": {
        "id": "4b457ee7"
      },
      "outputs": [],
      "source": [
        "# User message to the LLM\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"Find the last three messages from the account named @togethercompute on X/Twitter and summarize them in one sentence. Make it funny!\",\n",
        "  }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "396cabd1",
      "metadata": {
        "id": "396cabd1"
      },
      "source": [
        "We'll send this message to our LLM hosted on TogetherAI.\n",
        "\n",
        "Because we have tools provided by Toolhouse we can skip having to code our own Search X parser.\n",
        "The tools are hosted and maintained by the Toolhouse team and are optimized for LLM usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08148e22",
      "metadata": {
        "id": "08148e22"
      },
      "outputs": [],
      "source": [
        "# TogetherAI response with Toolhouse tools\n",
        "response = client.chat.completions.create(\n",
        "  model=MODEL,\n",
        "  messages=messages,\n",
        "  # Passes the tools to the model\n",
        "  tools=th.get_tools(),\n",
        ")\n",
        "\n",
        "print(\"The LLM autonomously decides to use the following tool(s) to achieve its goal of finding data on X/Twitter\")\n",
        "print(\"Tool selected: \", response.choices[0].message.tool_calls[0].function.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f279224f",
      "metadata": {
        "id": "f279224f"
      },
      "source": [
        "As you can see from the output above, the LLM properly identified that we'd like to invoke the 'search_x' tool\n",
        "\n",
        "You can see the arguments it generated by running the code below, it will break iut down by ID, type and function arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8505b93d",
      "metadata": {
        "id": "8505b93d"
      },
      "outputs": [],
      "source": [
        "tools_called = response.choices[0].message.tool_calls\n",
        "for tool_called in tools_called:\n",
        "    print(f\"ID: {tool_called.id}\")\n",
        "    print(f\"Type: {tool_called.type}\")\n",
        "    print(f\"Function: {tool_called.function}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e13ddcaf",
      "metadata": {
        "id": "e13ddcaf"
      },
      "source": [
        "# 4. Execute Tool Call"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0a0978f",
      "metadata": {
        "id": "a0a0978f"
      },
      "source": [
        "Now that we learned how the LLM can generate arguments to pass to a function we'll have to **run the function with those arguments**.\n",
        "\n",
        "The tool gets executed via the `run_tools` command, with the parameters that were identified in the previous LLM call.\n",
        "\n",
        "\n",
        "After that we will get the result, and append it to the context, the `messages` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "355f0f50",
      "metadata": {
        "id": "355f0f50"
      },
      "outputs": [],
      "source": [
        "tool_run = th.run_tools(response)\n",
        "messages += tool_run\n",
        "\n",
        "# Here's what the messages variable contains now\n",
        "print(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what our messages list looks like:"
      ],
      "metadata": {
        "id": "aOTqRaFe7xMa"
      },
      "id": "aOTqRaFe7xMa"
    },
    {
      "cell_type": "code",
      "source": [
        "for message in tool_run:\n",
        "  print(f\"Role: {message['role']}\")\n",
        "  if 'tool_calls' in message:\n",
        "    print(f\"Tool Calls: {message['tool_calls']}\")\n",
        "  if 'tool_call_id' in message:\n",
        "    print(f\"Tool Call ID: {message['tool_call_id']}\")\n",
        "    print(f\"Content: {message['content']}\")\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "ZqeSLyYhJRMc"
      },
      "id": "ZqeSLyYhJRMc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "84ceb341",
      "metadata": {
        "id": "84ceb341"
      },
      "source": [
        "We're now going to pass the whole message chain to the LLM. The LLM will generate a proper response based on our initial prompt:\n",
        "\n",
        "```\n",
        "Find the last three messages from the account named @togethercompute on X/Twitter and summarize them in one sentence. Make it funny!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 The Funny Summary"
      ],
      "metadata": {
        "id": "8rJ0agFQ-v6a"
      },
      "id": "8rJ0agFQ-v6a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84f24416",
      "metadata": {
        "id": "84f24416"
      },
      "outputs": [],
      "source": [
        "summary_response = client.chat.completions.create(\n",
        "  model=MODEL,\n",
        "  messages=messages\n",
        ")\n",
        "\n",
        "print('LLM RESPONSE:', summary_response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add this last response - a funny summary of three posts retrieved from X - to the history of messages.\n",
        "\n",
        "To achieve this we're combining two arrays into a new variable called `new_messages`."
      ],
      "metadata": {
        "id": "1sJuKocpNd0_"
      },
      "id": "1sJuKocpNd0_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3b5513b",
      "metadata": {
        "id": "f3b5513b"
      },
      "outputs": [],
      "source": [
        "new_messages = messages + [\n",
        "  {\n",
        "    \"role\": \"assistant\",\n",
        "    \"content\": summary_response.choices[0].message.content,\n",
        "  }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we're going to give a new command to the LLM - generate us an image from this funny caption using the tool provided by Toolhouse.\n",
        "\n",
        "This tool uses Flux - an image model hosted on TogetherAI, then hosts it to a storage location on the img.toolhouse.ai subdomain.\n",
        "\n",
        "> This shows you that a tool can be used to perform multiple action at once (image generation and hosting), but it's up to you - the user and creator of tools - how to set up every tool for best agentic use.\n",
        "\n",
        "We've installed this tool in step 1."
      ],
      "metadata": {
        "id": "T9K6hrDkbbIP"
      },
      "id": "T9K6hrDkbbIP"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "generate_image_request = new_messages + [\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"Generate an image from the funny summary\"\n",
        "  }\n",
        "]\n",
        "\n",
        "# Let's ask the LLM to generate the image\n",
        "image_response = client.chat.completions.create(\n",
        "  model=MODEL,\n",
        "  messages=generate_image_request,\n",
        "  tools=th.get_tools(),\n",
        ")\n",
        "tool_run = th.run_tools(image_response)"
      ],
      "metadata": {
        "id": "rQtHdIjObpZA"
      },
      "id": "rQtHdIjObpZA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the LLM generated an image and stored it, we just want it to let the user know that the image is ready and can be viewed.\n",
        "\n",
        "We're going to - yet again - ask the LLM to take the output of the function call and generate a response."
      ],
      "metadata": {
        "id": "-9RI1IWZbX4I"
      },
      "id": "-9RI1IWZbX4I"
    },
    {
      "cell_type": "code",
      "source": [
        "image_messages = generate_image_request + tool_run\n",
        "\n",
        "last_response = client.chat.completions.create(\n",
        "  model=MODEL,\n",
        "  messages=image_messages,\n",
        "  tools=th.get_tools(),\n",
        ")\n",
        "\n",
        "print('LLM RESPONSE:', last_response.choices[0].message.content)\n"
      ],
      "metadata": {
        "id": "Gjk6rNvfXanT"
      },
      "id": "Gjk6rNvfXanT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And there you have it, if you have managed to run all the cells in the correct order you should see a message with a valid URL. If you click it you will be able to see the generated image! The image is pretty random and truly depends on what \"funny summary\" we generated in step 4.1"
      ],
      "metadata": {
        "id": "TwDJcIG_cN46"
      },
      "id": "TwDJcIG_cN46"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
